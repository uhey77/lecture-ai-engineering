{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSpnWBP5ELSI"
      },
      "source": [
        "# å®Ÿè·µæ¼”ç¿’ Day 1ï¼šstreamlitã¨FastAPIã®ãƒ‡ãƒ¢\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã®å†…å®¹ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
        "\n",
        "- å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ç’°å¢ƒè¨­å®š\n",
        "- Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸStreamlitã®ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒª\n",
        "- FastAPIã¨ngrokã‚’ä½¿ç”¨ã—ãŸAPIã®å…¬é–‹æ–¹æ³•\n",
        "\n",
        "æ¼”ç¿’ã‚’å§‹ã‚ã‚‹å‰ã«ã€HuggingFaceã¨ngrokã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã—ã€\n",
        "ãã‚Œãã‚Œã®APIãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "\n",
        "æ¼”ç¿’ã®æ™‚é–“ã§ã¯ã€ä»¥ä¸‹ã®3ã¤ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é †ã«èª¬æ˜ã—ã¾ã™ã€‚\n",
        "\n",
        "1. 01_streamlit_UI\n",
        "2. 02_streamlit_app\n",
        "3. 03_FastAPI\n",
        "\n",
        "2ã¤ç›®ã‚„3ã¤ç›®ã‹ã‚‰ã§ã‚‚å§‹ã‚ã‚‰ã‚Œã‚‹æ§˜ã«ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "å¾©ç¿’ã®éš›ã«ã‚‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å½¹ç«‹ã¦ã¦ã„ãŸã ã‘ã‚Œã°ã¨æ€ã„ã¾ã™ã€‚\n",
        "\n",
        "### æ³¨æ„äº‹é …\n",
        "ã€Œ02_streamlit_appã€ã¨ã€Œ03_FastAPIã€ã§ã¯ã€GPUã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‰ã‚’å®Ÿè¡Œã™ã‚‹éš›ã¯ã€Google Colabç”»é¢ä¸Šã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œç·¨é›†ã€â†’ ã€Œãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®è¨­å®šã€\n",
        "\n",
        "ã€Œãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ãƒ¼ã€ã®é …ç›®ã®ä¸­ã‹ã‚‰ã€ã€ŒT4 GPUã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€ŒCPUã€ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhtHkJOgELSL"
      },
      "source": [
        "# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆ1~3å…±æœ‰ï¼‰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-FjBp4MMQHM"
      },
      "source": [
        "GitHubã‹ã‚‰æ¼”ç¿’ç”¨ã®ã‚³ãƒ¼ãƒ‰ã‚’Cloneã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AIXMavdDEP8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fafed51-a8f6-4147-bb25-43e65ac3df73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lecture-ai-engineering'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 41 (delta 7), reused 5 (delta 5), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (41/41), 34.04 KiB | 571.00 KiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC8n7yZ_vs1K"
      },
      "source": [
        "å¿…è¦ãªAPIãƒˆãƒ¼ã‚¯ãƒ³ã‚’.envã«è¨­å®šã—ã¾ã™ã€‚\n",
        "\n",
        "ã€Œlecture-ai-engineering/day1ã€ã®é…ä¸‹ã«ã€ã€Œ.env_templateã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã®ãŸã‚è¡¨ç¤ºã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ç”»é¢å·¦å´ã®ã‚ã‚‹ã€ç›®ã®ã‚¢ã‚¤ã‚³ãƒ³ã®ã€Œéš ã—ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡¨ç¤ºã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ã€Œ.env_templateã€ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã€Œ.envã€ã«å¤‰æ›´ã—ã¾ã™ã€‚ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªä¸­èº«ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "\n",
        "```\n",
        "HUGGINGFACE_TOKEN=\"hf-********\"\n",
        "NGROK_TOKEN=\"********\"\n",
        "```\n",
        "ãƒ€ãƒ–ãƒ«ã‚¯ã‚ªãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§å›²ã¾ã‚ŒãŸæ–‡å­—åˆ—ã‚’Huggingfaceã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã€ngrokã®èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã§æ›¸ãå¤‰ãˆã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ãã‚Œãã‚Œã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒä½œæˆæ¸ˆã¿ã§ã‚ã‚Œã°ã€ä»¥ä¸‹ã®URLã‹ã‚‰ãã‚Œãã‚Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã§ãã¾ã™ã€‚\n",
        "\n",
        "- Huggingfaceã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³\n",
        "https://huggingface.co/docs/hub/security-tokens\n",
        "\n",
        "- ngrokã®èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³\n",
        "https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "æ›¸ãæ›ãˆãŸã‚‰ã€ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã®PCã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "ã€Œ01_streamlit_UIã€ã‹ã‚‰ã€Œ02_streamlit_appã€ã¸é€²ã‚€éš›ã«ã€CPUã‹ã‚‰GPUã®åˆ©ç”¨ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä¸€åº¦åˆ‡ã‚Œã¦ã—ã¾ã„ã¾ã™ã€‚\n",
        "\n",
        "ãã®éš›ã«ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ãŸã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã¯å†ä½œæˆã™ã‚‹ã“ã¨ã«ãªã‚‹ã®ã§ã€ãã®æ‰‹é–“ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãŠãã¨è‰¯ã„ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py1BFS5RqcSS"
      },
      "source": [
        "ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦è¨­å®šã—ã¾ã™ã€‚æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã€æœ€çµ‚çš„ã«ã€ŒTrueã€ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚Œã°ã†ã¾ãèª­ã¿è¾¼ã‚ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bvEowFfg5lrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa8488d-aa6f-45b6-c7eb-0f06bd6a8165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "/content/lecture-ai-engineering/day1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "%cd /content/lecture-ai-engineering/day1\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç’°å¢ƒå¤‰æ•°ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã—ã¦ãŠãã‚³ãƒ¼ãƒ‰"
      ],
      "metadata": {
        "id": "fTE7nUlM160Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def save_env_backup():\n",
        "        env_backup = {\n",
        "                \"HUGGINGFACE_TOKEN\": os.environ.get(\"HUGGINGFACE_TOKEN\"),\n",
        "                \"NGROK_TOKEN\": os.environ.get(\"NGROK_TOKEN\")\n",
        "            }\n",
        "        with open('/content/env_backup.json', 'w') as f:\n",
        "              json.dump(env_backup, f)\n",
        "        print(\"ç’°å¢ƒå¤‰æ•°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡æ–­ã•ã‚ŒãŸå ´åˆã¯ restore_env_backup() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "        # ç’°å¢ƒå¤‰æ•°ã‚’å¾©å…ƒã™ã‚‹é–¢æ•°\n",
        "        def restore_env_backup():\n",
        "            try:\n",
        "                with open('/content/env_backup.json', 'r') as f:\n",
        "                    env_backup = json.load(f)\n",
        "\n",
        "                # ç’°å¢ƒå¤‰æ•°ã‚’å¾©å…ƒ\n",
        "                for key, value in env_backup.items():\n",
        "                    os.environ[key] = value\n",
        "\n",
        "                # .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ä½œæˆ\\n\",\n",
        "                with open('/content/lecture-ai-engineering/day1/.env', 'w') as f:\n",
        "                    for key, value in env_backup.items():\n",
        "                        f.write(f\"{key}={value}\")\n",
        "\n",
        "                print(\"ç’°å¢ƒå¤‰æ•°ã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚\")\n",
        "                return True\n",
        "            except FileNotFoundError:\n",
        "                print(\"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’æ‰‹å‹•ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚\")\n",
        "                return False\n",
        "\n",
        "        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ\n",
        "        save_env_backup()"
      ],
      "metadata": {
        "id": "BLkrs1sc2CTT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os0Yk6gaELSM"
      },
      "source": [
        "# 01_streamlit_UI\n",
        "\n",
        "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€Œ01_streamlit_UIã€ã«ç§»å‹•ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S28XgOm0ELSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113a90ce-059f-4644-cd6e-f361dd533d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/01_streamlit_UI\n"
          ]
        }
      ],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/01_streamlit_UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVp-aEIkELSM"
      },
      "source": [
        "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nBe41LFiELSN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install streamlit-option-menu streamlit-authenticator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æ–°ã—ã„ UI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è¿½åŠ **"
      ],
      "metadata": {
        "id": "tw3EGhkd4MZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile custom_ui.py\n",
        "\"\"\"\n",
        "custom_ui.py\n",
        "-----------------\n",
        "Streamlit ç”¨ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼†ã‚«ã‚¹ã‚¿ãƒ ãƒšãƒ¼ã‚¸éƒ¨å“ã€‚\n",
        "â€¢ ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠãƒ»ãƒ†ãƒ¼ãƒåˆ‡æ›¿\n",
        "â€¢ ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®ã‚«ãƒ¼ãƒ‰ã¨ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import time\n",
        "import streamlit as st\n",
        "from streamlit_option_menu import option_menu\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ã‚µã‚¤ãƒ‰ãƒãƒ¼\n",
        "# ----------------------------------------------------------------------\n",
        "_MENU_ITEMS = [\"ãƒ›ãƒ¼ãƒ \", \"åŸºæœ¬è¦ç´ \", \"ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\", \"å…¥åŠ›è¦ç´ \", \"ãƒ†ãƒ¼ãƒè¨­å®š\"]\n",
        "_ICONS = [\"house\", \"list-task\", \"columns\", \"input-cursor\", \"palette\"]\n",
        "\n",
        "\n",
        "def _apply_dark_theme(enable_dark: bool) -> None:\n",
        "    \"\"\"ãƒ€ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒã‚’ CSS ã§é©ç”¨ï¼è§£é™¤ã™ã‚‹ã€‚\"\"\"\n",
        "    if enable_dark:\n",
        "        st.markdown(\n",
        "            \"\"\"\n",
        "            <style>\n",
        "            .main {background-color: #0E1117; color: #FFFFFF;}\n",
        "            .sidebar .sidebar-content {background-color: #262730; color: #FFFFFF;}\n",
        "            </style>\n",
        "            \"\"\",\n",
        "            unsafe_allow_html=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_sidebar() -> str:\n",
        "    \"\"\"\n",
        "    ã‚«ã‚¹ã‚¿ãƒ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’ç”Ÿæˆã—ã€é¸æŠã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è¿”ã™ã€‚\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        ç¾åœ¨é¸æŠä¸­ã®ãƒšãƒ¼ã‚¸åã€‚\n",
        "    \"\"\"\n",
        "    with st.sidebar:\n",
        "        selected = option_menu(\n",
        "            menu_title=\"ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼\",\n",
        "            options=_MENU_ITEMS,\n",
        "            icons=_ICONS,\n",
        "            menu_icon=\"cast\",\n",
        "            default_index=0,\n",
        "        )\n",
        "\n",
        "        # ãƒ†ãƒ¼ãƒåˆ‡æ›¿ã‚¹ã‚¤ãƒƒãƒ\n",
        "        if \"light_mode\" not in st.session_state:\n",
        "            st.session_state.light_mode = True\n",
        "\n",
        "        if st.button(\"ğŸŒ“ ãƒ†ãƒ¼ãƒåˆ‡æ›¿\"):\n",
        "            st.session_state.light_mode = not st.session_state.light_mode\n",
        "\n",
        "    # ã‚µã‚¤ãƒ‰ãƒãƒ¼å¤–ã§ãƒ†ãƒ¼ãƒé©ç”¨ï¼ˆCSS ã¯ 1 åº¦ã®ã¿æŒ¿å…¥ï¼‰\n",
        "    _apply_dark_theme(not st.session_state.light_mode)\n",
        "\n",
        "    return selected\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
        "# ----------------------------------------------------------------------\n",
        "def page_hourglass() -> None:\n",
        "    \"\"\"ãƒ‡ãƒ¢ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹ãƒšãƒ¼ã‚¸ã€‚\"\"\"\n",
        "    st.subheader(\"æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º\")\n",
        "    progress_text = \"å‡¦ç†ä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„...\"\n",
        "    bar = st.progress(0, text=progress_text)\n",
        "    placeholder = st.empty()\n",
        "\n",
        "    for percent in range(100):\n",
        "        time.sleep(0.02)\n",
        "        bar.progress(percent + 1, text=f\"{progress_text} ({percent + 1}%)\")\n",
        "        if percent % 10 == 0:\n",
        "            placeholder.info(f\"Step {percent // 10 + 1}/10 å®Œäº†\")\n",
        "\n",
        "    bar.empty()\n",
        "    placeholder.empty()\n",
        "    st.success(\"å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ãƒšãƒ¼ã‚¸åˆ‡æ›¿ãƒãƒ–\n",
        "# ----------------------------------------------------------------------\n",
        "def show_custom_pages(selected: str) -> str:\n",
        "    \"\"\"\n",
        "    é¸æŠã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã«å¯¾å¿œã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æç”»ã™ã‚‹ã€‚\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    selected : str\n",
        "        create_sidebar ã‹ã‚‰è¿”ã•ã‚ŒãŸãƒšãƒ¼ã‚¸åã€‚\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        åŒã˜å€¤ã‚’è¿”ã™ã ã‘ï¼ˆå‘¼ã³å‡ºã—å…ƒã§ã®å†åˆ©ç”¨ç”¨ï¼‰ã€‚\n",
        "    \"\"\"\n",
        "    if selected == \"ãƒ›ãƒ¼ãƒ \":\n",
        "        _render_home()\n",
        "\n",
        "    # ãã®ã»ã‹ã®ãƒšãƒ¼ã‚¸ã¯ app.py ã§å‡¦ç†\n",
        "    return selected\n",
        "\n",
        "\n",
        "def _render_home() -> None:\n",
        "    \"\"\"ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‚’æç”»ã€‚\"\"\"\n",
        "    st.title(\"Streamlit UIãƒ‡ãƒ¢ï¼ˆæ”¹å–„ç‰ˆï¼‰\")\n",
        "    st.write(\n",
        "        \"\"\"\n",
        "        ã“ã®ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã¯ã€Streamlit ã®åŸºæœ¬çš„ãª UI è¦ç´ ã‚’ç´¹ä»‹ã™ã‚‹ã‚‚ã®ã§ã™ã€‚\n",
        "        ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ç•°ãªã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ã€ã•ã¾ã–ã¾ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ãŠè©¦ã—ãã ã•ã„ã€‚\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # ã‚«ãƒ¼ãƒ‰é¢¨ã« 3 ã‚«ãƒ©ãƒ ã§æ©Ÿèƒ½æ¡ˆå†…\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.info(\"**åŸºæœ¬è¦ç´ **\\n\\nãƒ†ã‚­ã‚¹ãƒˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ¡ãƒ‡ã‚£ã‚¢ãªã©\")\n",
        "    with col2:\n",
        "        st.success(\"**ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ**\\n\\nåˆ—ã€ã‚¿ãƒ–ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãªã©\")\n",
        "    with col3:\n",
        "        st.warning(\"**å…¥åŠ›è¦ç´ **\\n\\nãƒœã‚¿ãƒ³ã€ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãªã©\")\n",
        "\n",
        "    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ãƒ‡ãƒ¢èµ·å‹•\n",
        "    if st.button(\"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãƒ‡ãƒ¢ã‚’è¡¨ç¤º\"):\n",
        "        page_hourglass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6aCDSi04WwI",
        "outputId": "97e9e1f5-e609-4134-f0a6-47e6ddc2e3c6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting custom_ui.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyw6VHaTELSN"
      },
      "source": [
        "ngrokã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€èªè¨¼ã‚’è¡Œã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "aYw1q0iXELSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393d9179-e244-4452-bfde-50956165de5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**app.pyã®æ›¸ãæ›ãˆ**"
      ],
      "metadata": {
        "id": "3x2rBm74meW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\"\"\"Streamlit UI Demo â€” cleaned & refactored\n",
        "------------------------------------------------\n",
        "A singleâ€‘file demo app showcasing basic Streamlit components.\n",
        "Split into small render_* functions for readability.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import io\n",
        "from datetime import date as dt_date\n",
        "from typing import Callable, Dict\n",
        "\n",
        "import altair as alt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "from custom_ui import create_sidebar, show_custom_pages\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# ãƒšãƒ¼ã‚¸å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def random_df(rows: int = 20, cols: int = 3, colnames: list[str] | None = None) -> pd.DataFrame:\n",
        "    \"\"\"ãƒ©ãƒ³ãƒ€ãƒ  DataFrame ã‚’ç”Ÿæˆã€‚\"\"\"\n",
        "    if colnames is None:\n",
        "        colnames = list(\"ABC\")[:cols]\n",
        "    return pd.DataFrame(np.random.randn(rows, cols), columns=colnames)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# å€‹åˆ¥ãƒšãƒ¼ã‚¸æç”»é–¢æ•°\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def render_basic() -> None:\n",
        "    \"\"\"åŸºæœ¬ UI è¦ç´ ãƒšãƒ¼ã‚¸\"\"\"\n",
        "    st.title(\"åŸºæœ¬è¦ç´ \")\n",
        "\n",
        "    # --- ãƒ†ã‚­ã‚¹ãƒˆé¡ ---\n",
        "    st.header(\"ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ \")\n",
        "    st.text(\"ã“ã‚Œã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚\")\n",
        "    st.markdown(\"**ã“ã‚Œã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚** *ã‚¤ã‚¿ãƒªãƒƒã‚¯* ã‚„ `ã‚³ãƒ¼ãƒ‰` ã‚‚ä½¿ãˆã¾ã™ã€‚\")\n",
        "    st.info(\"ã“ã‚Œã¯æƒ…å ±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚\")\n",
        "    st.warning(\"ã“ã‚Œã¯è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚\")\n",
        "    st.error(\"ã“ã‚Œã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚\")\n",
        "    st.success(\"ã“ã‚Œã¯æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚\")\n",
        "\n",
        "    # --- ãƒ¡ãƒ‡ã‚£ã‚¢è¦ç´  ---\n",
        "    st.header(\"ãƒ¡ãƒ‡ã‚£ã‚¢è¦ç´ \")\n",
        "    tab_chart, tab_df, tab_img = st.tabs([\"ğŸ“ˆ ãƒãƒ£ãƒ¼ãƒˆ\", \"ğŸ—ƒ DataFrame\", \"ğŸ–¼ ç”»åƒ\"])\n",
        "\n",
        "    with tab_chart:\n",
        "        st.subheader(\"æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•\")\n",
        "        chart_data = random_df()\n",
        "        st.line_chart(chart_data)\n",
        "\n",
        "        st.subheader(\"Altair ãƒãƒ£ãƒ¼ãƒˆ\")\n",
        "        c = (\n",
        "            alt.Chart(chart_data.reset_index())\n",
        "            .mark_circle()\n",
        "            .encode(x=\"index\", y=\"A\", size=\"B\", color=\"C\", tooltip=[\"A\", \"B\", \"C\"])\n",
        "            .interactive()\n",
        "        )\n",
        "        st.altair_chart(c, use_container_width=True)\n",
        "\n",
        "    with tab_df:\n",
        "        st.subheader(\"DataFrame ã®è¡¨ç¤º\")\n",
        "        df = pd.DataFrame({\n",
        "            \"åå‰\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "            \"å¹´é½¢\": [24, 42, 18, 31],\n",
        "            \"éƒ½å¸‚\": [\"æ±äº¬\", \"å¤§é˜ª\", \"äº¬éƒ½\", \"åå¤å±‹\"],\n",
        "        })\n",
        "        st.dataframe(df, use_container_width=True)\n",
        "        st.download_button(\"CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\", df.to_csv(index=False, encoding=\"utf-8-sig\"), \"sample_data.csv\", \"text/csv\")\n",
        "\n",
        "    with tab_img:\n",
        "        st.subheader(\"å‹•çš„ã«ç”Ÿæˆã—ãŸç”»åƒ\")\n",
        "        fig, ax = plt.subplots()\n",
        "        x = np.linspace(0, 10, 100)\n",
        "        ax.plot(x, np.sin(x))\n",
        "        ax.set_title(\"ã‚µã‚¤ãƒ³æ³¢\")\n",
        "        ax.grid(True)\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format=\"png\")\n",
        "        st.image(buf.getvalue(), caption=\"å‹•çš„ã«ç”Ÿæˆã•ã‚ŒãŸã‚µã‚¤ãƒ³æ³¢\", use_column_width=True)\n",
        "\n",
        "\n",
        "def render_layout() -> None:\n",
        "    \"\"\"ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ãƒšãƒ¼ã‚¸\"\"\"\n",
        "    st.title(\"ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ \")\n",
        "\n",
        "    st.header(\"ã‚«ãƒ©ãƒ ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\")\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.subheader(\"ã‚«ãƒ©ãƒ 1\")\n",
        "        st.image(\"https://via.placeholder.com/150\", caption=\"ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒ\")\n",
        "    with col2:\n",
        "        st.subheader(\"ã‚«ãƒ©ãƒ 2\")\n",
        "        st.metric(\"æ¸©åº¦\", \"28Â°C\", \"1.2Â°C\")\n",
        "    with col3:\n",
        "        st.subheader(\"ã‚«ãƒ©ãƒ 3\")\n",
        "        st.metric(\"æ¹¿åº¦\", \"65%\", \"-4%\", delta_color=\"inverse\")\n",
        "\n",
        "    st.header(\"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼\")\n",
        "    with st.expander(\"è©³ç´°ã‚’è¡¨ç¤º\"):\n",
        "        st.write(\"\"\"\n",
        "            ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€é•·ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ˜ã‚ŠãŸãŸã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "            ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¿…è¦ãªã¨ãã«å±•é–‹ã§ãã‚‹ãŸã‚ã€ç”»é¢ã‚¹ãƒšãƒ¼ã‚¹ã‚’ç¯€ç´„ã§ãã¾ã™ã€‚\n",
        "        \"\"\")\n",
        "        st.image(\"https://via.placeholder.com/400x200\", caption=\"å¤§ããªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒ\")\n",
        "\n",
        "    st.header(\"ã‚¿ãƒ–\")\n",
        "    tab1, tab2, tab3 = st.tabs([\"Tab 1\", \"Tab 2\", \"Tab 3\"])\n",
        "    with tab1:\n",
        "        st.bar_chart(random_df(10, 3, [\"X\", \"Y\", \"Z\"]))\n",
        "    with tab2:\n",
        "        st.line_chart(pd.DataFrame(np.sin(np.linspace(0, 10, 100))))\n",
        "    with tab3:\n",
        "        st.area_chart(random_df(10).cumsum())\n",
        "\n",
        "\n",
        "def render_inputs() -> None:\n",
        "    \"\"\"å…¥åŠ›è¦ç´ ãƒšãƒ¼ã‚¸\"\"\"\n",
        "    st.title(\"å…¥åŠ›è¦ç´ \")\n",
        "\n",
        "    # --- ãƒœã‚¿ãƒ³ ---\n",
        "    st.header(\"ãƒœã‚¿ãƒ³\")\n",
        "    if st.button(\"ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„\"):\n",
        "        st.success(\"ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã¾ã—ãŸï¼\")\n",
        "\n",
        "    # --- ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ ---\n",
        "    st.header(\"ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹\")\n",
        "    if st.checkbox(\"ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤º\"):\n",
        "        st.write(\"ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãŒã‚ªãƒ³ã«ãªã£ã¦ã„ã¾ã™ã€‚\")\n",
        "\n",
        "    # --- ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ ---\n",
        "    st.header(\"ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³\")\n",
        "    genre = st.radio(\"å¥½ããªéŸ³æ¥½ã‚¸ãƒ£ãƒ³ãƒ«ã¯ï¼Ÿ\", (\"ãƒ­ãƒƒã‚¯\", \"ãƒãƒƒãƒ—\", \"ã‚¸ãƒ£ã‚º\", \"ã‚¯ãƒ©ã‚·ãƒƒã‚¯\"))\n",
        "    if genre:\n",
        "        st.write(f\"ã‚ãªãŸã¯ {genre} ã‚’é¸æŠã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "    # --- ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ ---\n",
        "    st.header(\"ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹\")\n",
        "    color = st.selectbox(\"å¥½ããªè‰²ã¯ï¼Ÿ\", (\"èµ¤\", \"é’\", \"ç·‘\", \"é»„è‰²\"))\n",
        "    st.write(f\"ã‚ãªãŸãŒé¸ã‚“ã ã®ã¯: {color}\")\n",
        "\n",
        "    # --- ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ ---\n",
        "    st.header(\"ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ\")\n",
        "    fruits = st.multiselect(\n",
        "        \"å¥½ããªæœç‰©ã¯ï¼Ÿ\",\n",
        "        [\"ã‚Šã‚“ã”\", \"ãƒãƒŠãƒŠ\", \"ã‚ªãƒ¬ãƒ³ã‚¸\", \"ã¶ã©ã†\", \"ã„ã¡ã”\"],\n",
        "        default=[\"ã‚Šã‚“ã”\", \"ãƒãƒŠãƒŠ\"],\n",
        "    )\n",
        "    st.write(\"ã‚ãªãŸãŒé¸ã‚“ã ã®ã¯: \" + \", \".join(fruits))\n",
        "\n",
        "    # --- ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ ---\n",
        "    st.header(\"ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼\")\n",
        "    age = st.slider(\"ã‚ãªãŸã®å¹´é½¢ã¯ï¼Ÿ\", 0, 100, 25)\n",
        "    st.write(f\"ã‚ãªãŸã®å¹´é½¢: {age}æ­³\")\n",
        "\n",
        "    # --- ç¯„å›²ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ ---\n",
        "    values = st.slider(\"å€¤ã®ç¯„å›²ã‚’é¸æŠ:\", 0.0, 100.0, (25.0, 75.0))\n",
        "    st.write(f\"é¸æŠã•ã‚ŒãŸç¯„å›²: {values[0]} ã‹ã‚‰ {values[1]}\")\n",
        "\n",
        "    # --- æ—¥ä»˜å…¥åŠ› ---\n",
        "    st.header(\"æ—¥ä»˜å…¥åŠ›\")\n",
        "    birth = st.date_input(\"ç”Ÿå¹´æœˆæ—¥ã‚’é¸æŠã—ã¦ãã ã•ã„\", dt_date(2000, 1, 1))\n",
        "    st.write(f\"ã‚ãªãŸã®ç”Ÿå¹´æœˆæ—¥: {birth}\")\n",
        "\n",
        "    # --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ ---\n",
        "    st.header(\"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼\")\n",
        "    uploaded = st.file_uploader(\"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„\", type=[\"csv\", \"xlsx\", \"txt\", \"jpg\", \"png\"])\n",
        "    if uploaded is not None:\n",
        "        st.write(f\"ãƒ•ã‚¡ã‚¤ãƒ«å: {uploaded.name} â€” {uploaded.size} bytes\")\n",
        "        if uploaded.type.startswith(\"image\"):\n",
        "            st.image(uploaded, caption=\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ\", use_column_width=True)\n",
        "        elif uploaded.type == \"text/plain\":\n",
        "            string_data = io.StringIO(uploaded.getvalue().decode()).read()\n",
        "            st.text_area(\"ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹\", string_data, height=200)\n",
        "        elif uploaded.type == \"text/csv\":\n",
        "            st.dataframe(pd.read_csv(uploaded), use_container_width=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# ãƒ†ãƒ¼ãƒè¨­å®šãƒšãƒ¼ã‚¸ã¯ custom_ui.py å†…ã® CSS ã§åˆ¶å¾¡\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def main() -> None:\n",
        "    st.set_page_config(\n",
        "        page_title=\"Streamlit UIãƒ‡ãƒ¢ï¼ˆæ”¹å–„ç‰ˆï¼‰\",\n",
        "        page_icon=\"ğŸ§Š\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\",\n",
        "    )\n",
        "\n",
        "    selected = create_sidebar()\n",
        "    selected = show_custom_pages(selected)  # ãƒ›ãƒ¼ãƒ ãªã© custom_ui å´\n",
        "\n",
        "    page_table: Dict[str, Callable[[], None]] = {\n",
        "        \"åŸºæœ¬è¦ç´ \": render_basic,\n",
        "        \"ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\": render_layout,\n",
        "        \"å…¥åŠ›è¦ç´ \": render_inputs,\n",
        "    }\n",
        "\n",
        "    if selected in page_table:\n",
        "        page_table[selected]()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqPQcRxLmikL",
        "outputId": "b02a3da3-ceeb-465d-c8cd-56b833379189"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RssTcD_IELSN"
      },
      "source": [
        "ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "f-E7ucR6ELSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9621cd0b-919d-4d0d-92f0-6d4d50332606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å…¬é–‹URL: https://f89e-34-125-210-255.ngrok-free.app\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.210.255:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 12469 (\\N{KATAKANA LETTER SA}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 12452 (\\N{KATAKANA LETTER I}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 12531 (\\N{KATAKANA LETTER N}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 27874 (\\N{CJK UNIFIED IDEOGRAPH-6CE2}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "2025-04-21 05:45:37.838 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"å…¬é–‹URL: {public_url}\")\n",
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbYyXVFjELSN"
      },
      "source": [
        "å…¬é–‹URLã®å¾Œã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹URLã«ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€streamlitã®UIãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "app.pyã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚Œã¦ã„ã‚‹ç®‡æ‰€ã‚’ç·¨é›†ã™ã‚‹ã“ã¨ã§ã€UIãŒã©ã®æ§˜ã«å¤‰åŒ–ã™ã‚‹ã‹ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "streamlitã®å…¬å¼ãƒšãƒ¼ã‚¸ã«ã¯ã€ã‚®ãƒ£ãƒ©ãƒªãƒ¼ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "streamlitã‚’ä½¿ã†ã¨pythonã¨ã„ã†ä¸€ã¤ã®è¨€èªã§ã‚ã£ã¦ã‚‚ã€æ§˜ã€…ãªUIã‚’å®Ÿç¾ã§ãã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã¨æ€ã„ã¾ã™ã€‚\n",
        "\n",
        "https://streamlit.io/gallery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmtP5GLOELSN"
      },
      "source": [
        "å¾Œç‰‡ä»˜ã‘ã¨ã—ã¦ã€ä½¿ã†å¿…è¦ã®ãªã„ngrokã®ãƒˆãƒ³ãƒãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8Ek9QgahELSO"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-T8tFpyELSO"
      },
      "source": [
        "# 02_streamlit_app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqogFQKnELSO"
      },
      "source": [
        "\n",
        "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€Œ02_streamlit_appã€ã«ç§»å‹•ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "UeEjlJ7uELSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa30922-ceef-4b57-eb8a-2379a0cf0e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/02_streamlit_app\n"
          ]
        }
      ],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/02_streamlit_app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XUH2AstELSO"
      },
      "source": [
        "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "mDqvI4V3ELSO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO31umGZELSO"
      },
      "source": [
        "ngrokã¨huggigfaceã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€èªè¨¼ã‚’è¡Œã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jPxTiEWQELSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5099379-14d1-4b42-d6cd-208898c564d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `AIE_2` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `AIE_2`\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz4WrELLELSP"
      },
      "source": [
        "stramlitã§Huggingfaceã®ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’æ‰±ã†ãŸã‚ã«ã€streamlitç”¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.streamlitï¼‰ã‚’ä½œæˆã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®æƒ…å ±ã‚’æ ¼ç´ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "W184-a7qFP0W"
      },
      "outputs": [],
      "source": [
        "# .streamlit/secrets.toml ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
        "import os\n",
        "import toml\n",
        "\n",
        "# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºä¿\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "\n",
        "# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€\n",
        "secrets = {\n",
        "    \"huggingface\": {\n",
        "        \"token\": os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n",
        "    }\n",
        "}\n",
        "\n",
        "# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    toml.dump(secrets, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK0vI_xKELSP"
      },
      "source": [
        "ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã™ã€‚\n",
        "\n",
        "02_streamlit_appã§ã¯ã€Huggingfaceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€åˆå›èµ·å‹•ã«ã¯2åˆ†ç¨‹åº¦æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã“ã®å¾…ã¡æ™‚é–“ã‚’åˆ©ç”¨ã—ã¦ã€app.pyã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**metrics.py ã®æ›¸ãæ›ãˆ**"
      ],
      "metadata": {
        "id": "HhaRy8Bt6f4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile llm.py\n",
        "# import time\n",
        "# import numpy as np\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from textblob import TextBlob\n",
        "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "# import nltk\n",
        "# import math\n",
        "# import re\n",
        "\n",
        "# # å¿…è¦ãªNLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "# try:\n",
        "#     nltk.download('punkt', quiet=True)\n",
        "# except:\n",
        "#     pass\n",
        "\n",
        "# class MetricsCalculator:\n",
        "#     def __init__(self):\n",
        "#         self.vectorizer = TfidfVectorizer()\n",
        "#         self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#     def calculate_bleu_score(self, reference, candidate):\n",
        "#         \"\"\"BLEUã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (0-1, é«˜ã„ã»ã©è‰¯ã„)\"\"\"\n",
        "#         reference_tokens = nltk.word_tokenize(reference.lower())\n",
        "#         candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
        "#         return sentence_bleu([reference_tokens], candidate_tokens)\n",
        "\n",
        "#     def calculate_cosine_similarity(self, text1, text2):\n",
        "#         \"\"\"ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®— (0-1, é«˜ã„ã»ã©ä¼¼ã¦ã„ã‚‹)\"\"\"\n",
        "#         try:\n",
        "#             tfidf_matrix = self.vectorizer.fit_transform([text1, text2])\n",
        "#             return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "#         except:\n",
        "#             return 0.0\n",
        "\n",
        "#     def calculate_sentiment_score(self, text):\n",
        "#         \"\"\"æ„Ÿæƒ…åˆ†æã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— (-1: éå¸¸ã«ãƒã‚¬ãƒ†ã‚£ãƒ–, 1: éå¸¸ã«ãƒã‚¸ãƒ†ã‚£ãƒ–)\"\"\"\n",
        "#         return self.sentiment_analyzer.polarity_scores(text)[\"compound\"]\n",
        "\n",
        "#     def calculate_sentiment_metrics(self, text):\n",
        "#         \"\"\"è©³ç´°ãªæ„Ÿæƒ…åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—\"\"\"\n",
        "#         sentiment = TextBlob(text).sentiment\n",
        "#         vader_sentiment = self.sentiment_analyzer.polarity_scores(text)\n",
        "\n",
        "#         return {\n",
        "#             \"polarity\": sentiment.polarity,  # -1.0 to 1.0\n",
        "#             \"subjectivity\": sentiment.subjectivity,  # 0.0 to 1.0\n",
        "#             \"vader_compound\": vader_sentiment[\"compound\"],\n",
        "#             \"vader_negative\": vader_sentiment[\"neg\"],\n",
        "#             \"vader_neutral\": vader_sentiment[\"neu\"],\n",
        "#             \"vader_positive\": vader_sentiment[\"pos\"]\n",
        "#         }\n",
        "\n",
        "#     def calculate_perplexity(self, text, n=3):\n",
        "#         \"\"\"å˜ç´”ãªN-gramãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ããƒ†ã‚­ã‚¹ãƒˆã®è¤‡é›‘ã•æ¨å®šï¼ˆä½ã„ã»ã©è‡ªç„¶ï¼‰\"\"\"\n",
        "#         tokens = nltk.word_tokenize(text.lower())\n",
        "#         if len(tokens) < n:\n",
        "#             return float('inf')  # ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã‚‹å ´åˆ\n",
        "\n",
        "#         # N-gramã‚«ã‚¦ãƒ³ãƒˆ\n",
        "#         ngrams = {}\n",
        "#         for i in range(len(tokens) - n + 1):\n",
        "#             gram = ' '.join(tokens[i:i+n-1])\n",
        "#             next_token = tokens[i+n-1]\n",
        "\n",
        "#             if gram not in ngrams:\n",
        "#                 ngrams[gram] = {}\n",
        "\n",
        "#             if next_token not in ngrams[gram]:\n",
        "#                 ngrams[gram][next_token] = 0\n",
        "\n",
        "#             ngrams[gram][next_token] += 1\n",
        "\n",
        "#         # ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£è¨ˆç®—\n",
        "#         log_prob = 0.0\n",
        "#         for i in range(len(tokens) - n + 1):\n",
        "#             gram = ' '.join(tokens[i:i+n-1])\n",
        "#             next_token = tokens[i+n-1]\n",
        "\n",
        "#             if gram in ngrams and next_token in ngrams[gram]:\n",
        "#                 total = sum(ngrams[gram].values())\n",
        "#                 prob = ngrams[gram][next_token] / total\n",
        "#                 log_prob += math.log2(prob)\n",
        "#             else:\n",
        "#                 log_prob += math.log2(1e-10)  # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°\n",
        "\n",
        "#         perplexity = 2 ** (-log_prob / (len(tokens) - n + 1))\n",
        "#         return perplexity\n",
        "\n",
        "#     def calculate_response_time(self, start_time):\n",
        "#         \"\"\"å¿œç­”æ™‚é–“ã‚’è¨ˆç®—ï¼ˆç§’å˜ä½ï¼‰\"\"\"\n",
        "#         return time.time() - start_time\n",
        "\n",
        "#     def calculate_token_generation_speed(self, text, generation_time):\n",
        "#         \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦ã‚’è¨ˆç®—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³/ç§’ï¼‰\"\"\"\n",
        "#         if generation_time <= 0:\n",
        "#             return 0\n",
        "#         # ç°¡æ˜“çš„ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®šï¼ˆå®Ÿéš›ã«ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰\n",
        "#         token_count = len(re.findall(r'\\w+|[^\\w\\s]', text))\n",
        "#         return token_count / generation_time\n",
        "\n",
        "#     def calculate_all_metrics(self, reference, candidate, generation_time):\n",
        "#         \"\"\"ã™ã¹ã¦ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—\"\"\"\n",
        "#         metrics = {\n",
        "#             \"bleu_score\": self.calculate_bleu_score(reference, candidate),\n",
        "#             \"cosine_similarity\": self.calculate_cosine_similarity(reference, candidate),\n",
        "#             \"sentiment\": self.calculate_sentiment_metrics(candidate),\n",
        "#             \"response_time\": generation_time,\n",
        "#             \"token_generation_speed\": self.calculate_token_generation_speed(candidate, generation_time),\n",
        "#             \"perplexity\": self.calculate_perplexity(candidate)\n",
        "#         }\n",
        "#         return metrics\n",
        "\n",
        "# # ä½¿ç”¨ä¾‹\n",
        "# if __name__ == \"__main__\":\n",
        "#     calculator = MetricsCalculator()\n",
        "#     reference = \"ã“ã‚Œã¯å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚\"\n",
        "#     candidate = \"ã“ã‚Œã¯ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚\"\n",
        "#     start_time = time.time()\n",
        "#     time.sleep(1)  # ç”Ÿæˆæ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "#     generation_time = time.time() - start_time\n",
        "\n",
        "#     metrics = calculator.calculate_all_metrics(reference, candidate, generation_time)\n",
        "#     print(metrics)"
      ],
      "metadata": {
        "id": "hBCnjqZ26ktb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**llm.pyã®æ›¸ãæ›ãˆ**"
      ],
      "metadata": {
        "id": "jJahIGtm6crr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile llm.py\n",
        "# import streamlit as st\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "# from transformers_stream_generator import init_streamer\n",
        "# import time\n",
        "# from config import MODEL_NAME, STREAM_OUTPUT, ENABLE_CACHING\n",
        "\n",
        "# class LLMGenerator:\n",
        "#     def __init__(self):\n",
        "#         self.model = None\n",
        "#         self.tokenizer = None\n",
        "#         self.load_model()\n",
        "\n",
        "#     @st.cache_resource\n",
        "#     def load_model_cached(_self):\n",
        "#         \"\"\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹\"\"\"\n",
        "#         print(f\"ãƒ¢ãƒ‡ãƒ« {MODEL_NAME} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "\n",
        "#         # é‡å­åŒ–è¨­å®š\n",
        "#         quantization_config = BitsAndBytesConfig(\n",
        "#             load_in_4bit=True,\n",
        "#             bnb_4bit_compute_dtype=torch.float16\n",
        "#         )\n",
        "\n",
        "#         # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "#         tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "#         # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆ4bité‡å­åŒ–ï¼‰\n",
        "#         model = AutoModelForCausalLM.from_pretrained(\n",
        "#             MODEL_NAME,\n",
        "#             quantization_config=quantization_config,\n",
        "#             device_map=\"auto\",\n",
        "#             torch_dtype=torch.float16\n",
        "#         )\n",
        "\n",
        "#         return model, tokenizer\n",
        "\n",
        "#     def load_model(self):\n",
        "#         \"\"\"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼‰\"\"\"\n",
        "#         if ENABLE_CACHING:\n",
        "#             self.model, self.tokenizer = self.load_model_cached()\n",
        "#         else:\n",
        "#             print(f\"ãƒ¢ãƒ‡ãƒ« {MODEL_NAME} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "\n",
        "#             # é‡å­åŒ–è¨­å®š\n",
        "#             quantization_config = BitsAndBytesConfig(\n",
        "#                 load_in_4bit=True,\n",
        "#                 bnb_4bit_compute_dtype=torch.float16\n",
        "#             )\n",
        "\n",
        "#             # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "#             self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "#             # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆ4bité‡å­åŒ–ï¼‰\n",
        "#             self.model = AutoModelForCausalLM.from_pretrained(\n",
        "#                 MODEL_NAME,\n",
        "#                 quantization_config=quantization_config,\n",
        "#                 device_map=\"auto\",\n",
        "#                 torch_dtype=torch.float16\n",
        "#             )\n",
        "\n",
        "#     def generate_text(self, prompt, max_length=512, temperature=0.7, stream_handler=None):\n",
        "#         \"\"\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚ã‚Š/ãªã—ä¸¡æ–¹å¯¾å¿œï¼‰\"\"\"\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         try:\n",
        "#             # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
        "#             if \"meta-llama\" in MODEL_NAME.lower():\n",
        "#                 formatted_prompt = f\"<human>: {prompt}\\n<assistant>: \"\n",
        "#             else:\n",
        "#                 formatted_prompt = f\"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}\\nã‚·ã‚¹ãƒ†ãƒ : \"\n",
        "\n",
        "#             # å…¥åŠ›ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
        "#             inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "#             generation_args = {\n",
        "#                 \"max_new_tokens\": max_length,\n",
        "#                 \"temperature\": temperature,\n",
        "#                 \"top_p\": 0.95,\n",
        "#                 \"top_k\": 50,\n",
        "#                 \"repetition_penalty\": 1.1,\n",
        "#                 \"do_sample\": temperature > 0.1,\n",
        "#                 \"pad_token_id\": self.tokenizer.eos_token_id\n",
        "#             }\n",
        "\n",
        "#             if STREAM_OUTPUT and stream_handler:\n",
        "#                 # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ\n",
        "#                 streamer = init_streamer(self.model, self.tokenizer)\n",
        "\n",
        "#                 # ç”Ÿæˆé–‹å§‹ï¼ˆéåŒæœŸï¼‰\n",
        "#                 generation_task = self.model.generate(\n",
        "#                     **inputs,\n",
        "#                     streamer=streamer,\n",
        "#                     **generation_args\n",
        "#                 )\n",
        "\n",
        "#                 # ã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã¦ãƒãƒ³ãƒ‰ãƒ©ã«æ¸¡ã™\n",
        "#                 generated_text = \"\"\n",
        "#                 for new_text in streamer:\n",
        "#                     generated_text += new_text\n",
        "#                     if stream_handler:\n",
        "#                         stream_handler(new_text)\n",
        "\n",
        "#                 # ç”Ÿæˆå®Œäº†ã‚’å¾…æ©Ÿ\n",
        "#                 try:\n",
        "#                     generation_task.result()\n",
        "#                 except:\n",
        "#                     pass\n",
        "\n",
        "#             else:\n",
        "#                 # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ\n",
        "#                 output = self.model.generate(\n",
        "#                     **inputs,\n",
        "#                     **generation_args\n",
        "#                 )\n",
        "\n",
        "#                 # å‡ºåŠ›ã®ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
        "#                 generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "#                 # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’å‰Šé™¤\n",
        "#                 generated_text = generated_text.replace(formatted_prompt, \"\")\n",
        "\n",
        "#             generation_time = time.time() - start_time\n",
        "#             return generated_text, generation_time\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
        "#             generation_time = time.time() - start_time\n",
        "#             return f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\", generation_time\n",
        "\n",
        "# # ä½¿ç”¨ä¾‹\n",
        "# if __name__ == \"__main__\":\n",
        "#     generator = LLMGenerator()\n",
        "#     response, gen_time = generator.generate_text(\"ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ\")\n",
        "#     print(f\"å¿œç­”: {response}\")\n",
        "#     print(f\"ç”Ÿæˆæ™‚é–“: {gen_time:.2f}ç§’\")"
      ],
      "metadata": {
        "id": "APfi3UtA6HcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**custom_ui.py è¿½åŠ **"
      ],
      "metadata": {
        "id": "f1IJsTLP6v2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile custom_ui.py\n",
        "# import streamlit as st\n",
        "# from streamlit_option_menu import option_menu\n",
        "\n",
        "# def create_sidebar():\n",
        "#     \"\"\"ã‚«ã‚¹ã‚¿ãƒ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’ä½œæˆã™ã‚‹é–¢æ•°\"\"\"\n",
        "#     with st.sidebar:\n",
        "#         selected = option_menu(\n",
        "#             \"ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼\",\n",
        "#             [\"ãƒ›ãƒ¼ãƒ \", \"åŸºæœ¬è¦ç´ \", \"ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\", \"å…¥åŠ›è¦ç´ \", \"ãƒ†ãƒ¼ãƒè¨­å®š\"],\n",
        "#             icons=[\"house\", \"list-task\", \"columns\", \"input-cursor\", \"palette\"],\n",
        "#             menu_icon=\"cast\",\n",
        "#             default_index=0,\n",
        "#         )\n",
        "\n",
        "#         # ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰/ãƒ©ã‚¤ãƒˆãƒ¢ãƒ¼ãƒ‰ã®åˆ‡ã‚Šæ›¿ãˆ\n",
        "#         if \"light_mode\" not in st.session_state:\n",
        "#             st.session_state.light_mode = True\n",
        "\n",
        "#         if st.button(\"ğŸŒ“ ãƒ†ãƒ¼ãƒåˆ‡æ›¿\"):\n",
        "#             st.session_state.light_mode = not st.session_state.light_mode\n",
        "\n",
        "#         # ã‚«ã‚¹ã‚¿ãƒ CSS\n",
        "#         if not st.session_state.light_mode:\n",
        "#             st.markdown(\"\"\"\n",
        "#             <style>\n",
        "#             .main {background-color: #0E1117; color: white;}\n",
        "#             .sidebar .sidebar-content {background-color: #262730; color: white;}\n",
        "#             </style>\n",
        "#             \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "#         return selected\n",
        "\n",
        "# def page_hourglass():\n",
        "#     \"\"\"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãƒ‡ãƒ¢ãƒšãƒ¼ã‚¸\"\"\"\n",
        "#     import time\n",
        "\n",
        "#     st.subheader(\"æ”¹å–„ã•ã‚ŒãŸãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º\")\n",
        "#     progress_text = \"å‡¦ç†ä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„...\"\n",
        "#     my_bar = st.progress(0, text=progress_text)\n",
        "#     placeholder = st.empty()\n",
        "\n",
        "#     for percent_complete in range(100):\n",
        "#         time.sleep(0.02)\n",
        "#         my_bar.progress(percent_complete + 1, text=f\"{progress_text} ({percent_complete+1}%)\")\n",
        "#         if percent_complete % 10 == 0:\n",
        "#             placeholder.info(f\"Step {percent_complete // 10 + 1}/10 å®Œäº†\")\n",
        "\n",
        "#     my_bar.empty()\n",
        "#     placeholder.empty()\n",
        "#     st.success(\"å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
        "\n",
        "# def show_custom_pages(selected):\n",
        "#     \"\"\"é¸æŠã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã«åŸºã¥ã„ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¡¨ç¤º\"\"\"\n",
        "#     if selected == \"ãƒ›ãƒ¼ãƒ \":\n",
        "#         st.title(\"Streamlit UIãƒ‡ãƒ¢ï¼ˆæ”¹å–„ç‰ˆï¼‰\")\n",
        "#         st.write(\"\"\"ã“ã®ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã¯ã€Streamlitã®åŸºæœ¬çš„ãªUIè¦ç´ ã‚’ç´¹ä»‹ã™ã‚‹ã‚‚ã®ã§ã™ã€‚\n",
        "#         ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ç•°ãªã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ã€æ§˜ã€…ãªStreamlitã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚\"\"\")\n",
        "\n",
        "#         # ã‚«ãƒ¼ãƒ‰è¦ç´ ã®è¿½åŠ \n",
        "#         col1, col2, col3 = st.columns(3)\n",
        "#         with col1:\n",
        "#             st.info(\"**åŸºæœ¬è¦ç´ **\\n\\nãƒ†ã‚­ã‚¹ãƒˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ¡ãƒ‡ã‚£ã‚¢ãªã©ã®åŸºæœ¬çš„ãªUIè¦ç´ \")\n",
        "#         with col2:\n",
        "#             st.success(\"**ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ**\\n\\nåˆ—ã€ã‚¿ãƒ–ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãªã©ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³\")\n",
        "#         with col3:\n",
        "#             st.warning(\"**å…¥åŠ›è¦ç´ **\\n\\nãƒœã‚¿ãƒ³ã€ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãªã©ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¦ç´ \")\n",
        "\n",
        "#         # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãƒ‡ãƒ¢ã®è¡¨ç¤º\n",
        "#         if st.button(\"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãƒ‡ãƒ¢ã‚’è¡¨ç¤º\"):\n",
        "#             page_hourglass()\n",
        "\n",
        "#     # ä»–ã®ãƒšãƒ¼ã‚¸ã®å®Ÿè£…ã¯app.pyã«ä»»ã›ã‚‹\n",
        "#     return selected"
      ],
      "metadata": {
        "id": "O-soh1H-60kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**database.py æ›¸ãæ›ãˆ**"
      ],
      "metadata": {
        "id": "zoRyejCN7IYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile database.py\n",
        "# import sqlite3\n",
        "# import json\n",
        "# import time\n",
        "# from datetime import datetime\n",
        "\n",
        "# class ChatDatabase:\n",
        "#     def __init__(self, db_file):\n",
        "#         self.db_file = db_file\n",
        "#         self.initialize_db()\n",
        "\n",
        "#     def initialize_db(self):\n",
        "#         \"\"\"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆæœŸåŒ–\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«\n",
        "#         cursor.execute('''\n",
        "#         CREATE TABLE IF NOT EXISTS chat_history (\n",
        "#             id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#             session_id TEXT,\n",
        "#             timestamp TEXT,\n",
        "#             user_input TEXT,\n",
        "#             model_response TEXT,\n",
        "#             response_time REAL,\n",
        "#             metrics TEXT\n",
        "#         )\n",
        "#         ''')\n",
        "\n",
        "#         # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«\n",
        "#         cursor.execute('''\n",
        "#         CREATE TABLE IF NOT EXISTS feedback (\n",
        "#             id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#             chat_id INTEGER,\n",
        "#             rating INTEGER,\n",
        "#             feedback_text TEXT,\n",
        "#             timestamp TEXT,\n",
        "#             FOREIGN KEY (chat_id) REFERENCES chat_history (id)\n",
        "#         )\n",
        "#         ''')\n",
        "\n",
        "#         # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæ–°è¦è¿½åŠ ï¼‰\n",
        "#         cursor.execute('''\n",
        "#         CREATE TABLE IF NOT EXISTS error_logs (\n",
        "#             id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#             timestamp TEXT,\n",
        "#             error_type TEXT,\n",
        "#             error_message TEXT,\n",
        "#             stack_trace TEXT,\n",
        "#             input_data TEXT\n",
        "#         )\n",
        "#         ''')\n",
        "\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#     def save_chat(self, session_id, user_input, model_response, response_time, metrics=None):\n",
        "#         \"\"\"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         timestamp = datetime.now().isoformat()\n",
        "#         metrics_json = json.dumps(metrics) if metrics else \"{}\"\n",
        "\n",
        "#         cursor.execute(\n",
        "#             \"INSERT INTO chat_history (session_id, timestamp, user_input, model_response, response_time, metrics) VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "#             (session_id, timestamp, user_input, model_response, response_time, metrics_json)\n",
        "#         )\n",
        "\n",
        "#         chat_id = cursor.lastrowid\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#         return chat_id\n",
        "\n",
        "#     def save_feedback(self, chat_id, rating, feedback_text=\"\"):\n",
        "#         \"\"\"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¿å­˜\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         timestamp = datetime.now().isoformat()\n",
        "\n",
        "#         cursor.execute(\n",
        "#             \"INSERT INTO feedback (chat_id, rating, feedback_text, timestamp) VALUES (?, ?, ?, ?)\",\n",
        "#             (chat_id, rating, feedback_text, timestamp)\n",
        "#         )\n",
        "\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#     def get_chat_history(self, session_id=None, limit=10, offset=0):\n",
        "#         \"\"\"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å–å¾—\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         conn.row_factory = sqlite3.Row\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         if session_id:\n",
        "#             cursor.execute(\n",
        "#                 \"SELECT * FROM chat_history WHERE session_id = ? ORDER BY timestamp DESC LIMIT ? OFFSET ?\",\n",
        "#                 (session_id, limit, offset)\n",
        "#             )\n",
        "#         else:\n",
        "#             cursor.execute(\n",
        "#                 \"SELECT * FROM chat_history ORDER BY timestamp DESC LIMIT ? OFFSET ?\",\n",
        "#                 (limit, offset)\n",
        "#             )\n",
        "\n",
        "#         rows = cursor.fetchall()\n",
        "#         history = []\n",
        "\n",
        "#         for row in rows:\n",
        "#             feedback_cursor = conn.cursor()\n",
        "#             feedback_cursor.execute(\n",
        "#                 \"SELECT rating, feedback_text FROM feedback WHERE chat_id = ?\",\n",
        "#                 (row['id'],)\n",
        "#             )\n",
        "#             feedback = feedback_cursor.fetchone()\n",
        "\n",
        "#             chat_item = dict(row)\n",
        "#             if feedback:\n",
        "#                 chat_item['feedback_rating'] = feedback['rating']\n",
        "#                 chat_item['feedback_text'] = feedback['feedback_text']\n",
        "#             else:\n",
        "#                 chat_item['feedback_rating'] = None\n",
        "#                 chat_item['feedback_text'] = None\n",
        "\n",
        "#             try:\n",
        "#                 chat_item['metrics'] = json.loads(chat_item['metrics'])\n",
        "#             except:\n",
        "#                 chat_item['metrics'] = {}\n",
        "\n",
        "#             history.append(chat_item)\n",
        "\n",
        "#         conn.close()\n",
        "#         return history\n",
        "\n",
        "#     def get_total_chat_count(self, session_id=None):\n",
        "#         \"\"\"ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç·æ•°ã‚’å–å¾—\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         if session_id:\n",
        "#             cursor.execute(\n",
        "#                 \"SELECT COUNT(*) FROM chat_history WHERE session_id = ?\",\n",
        "#                 (session_id,)\n",
        "#             )\n",
        "#         else:\n",
        "#             cursor.execute(\"SELECT COUNT(*) FROM chat_history\")\n",
        "\n",
        "#         count = cursor.fetchone()[0]\n",
        "#         conn.close()\n",
        "#         return count\n",
        "\n",
        "#     def log_error(self, error_type, error_message, stack_trace=\"\", input_data=\"\"):\n",
        "#         \"\"\"ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆæ–°è¦è¿½åŠ ï¼‰\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         timestamp = datetime.now().isoformat()\n",
        "\n",
        "#         cursor.execute(\n",
        "#             \"INSERT INTO error_logs (timestamp, error_type, error_message, stack_trace, input_data) VALUES (?, ?, ?, ?, ?)\",\n",
        "#             (timestamp, error_type, error_message, stack_trace, input_data)\n",
        "#         )\n",
        "\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#     def get_statistics(self):\n",
        "#         \"\"\"ãƒãƒ£ãƒƒãƒˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆæ–°è¦è¿½åŠ ï¼‰\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         conn.row_factory = sqlite3.Row\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         # å¹³å‡å¿œç­”æ™‚é–“\n",
        "#         cursor.execute(\"SELECT AVG(response_time) as avg_response_time FROM chat_history\")\n",
        "#         avg_response_time = cursor.fetchone()['avg_response_time'] or 0\n",
        "\n",
        "#         # ç·ãƒãƒ£ãƒƒãƒˆæ•°\n",
        "#         cursor.execute(\"SELECT COUNT(*) as total_chats FROM chat_history\")\n",
        "#         total_chats = cursor.fetchone()['total_chats'] or 0\n",
        "\n",
        "#         # å¹³å‡è©•ä¾¡\n",
        "#         cursor.execute(\"SELECT AVG(rating) as avg_rating FROM feedback\")\n",
        "#         avg_rating = cursor.fetchone()['avg_rating'] or 0\n",
        "\n",
        "#         # è©•ä¾¡åˆ†å¸ƒ\n",
        "#         cursor.execute(\"\"\"\n",
        "#             SELECT rating, COUNT(*) as count\n",
        "#             FROM feedback\n",
        "#             GROUP BY rating\n",
        "#             ORDER BY rating\n",
        "#         \"\"\")\n",
        "#         rating_distribution = {row['rating']: row['count'] for row in cursor.fetchall()}\n",
        "\n",
        "#         conn.close()\n",
        "\n",
        "#         return {\n",
        "#             'avg_response_time': avg_response_time,\n",
        "#             'total_chats': total_chats,\n",
        "#             'avg_rating': avg_rating,\n",
        "#             'rating_distribution': rating_distribution\n",
        "#         }\n",
        "\n",
        "# # ä½¿ç”¨ä¾‹\n",
        "# if __name__ == \"__main__\":\n",
        "#     db = ChatDatabase(\"test.db\")\n",
        "#     chat_id = db.save_chat(\"test_session\", \"ã“ã‚“ã«ã¡ã¯\", \"ã“ã‚“ã«ã¡ã¯ï¼\", 0.5, {\"bleu_score\": 0.8})\n",
        "#     db.save_feedback(chat_id, 5, \"ã¨ã¦ã‚‚è‰¯ã„å¿œç­”ã§ã—ãŸ\")\n",
        "#     history = db.get_chat_history(\"test_session\")\n",
        "#     print(history)"
      ],
      "metadata": {
        "id": "o-rIp1VF7L1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**app.py æ›¸ãæ›ãˆ**"
      ],
      "metadata": {
        "id": "Q1hK7yN07WwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import uuid\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import time\n",
        "# import io\n",
        "# import json\n",
        "# from datetime import datetime\n",
        "\n",
        "# from llm import LLMGenerator\n",
        "# from database import ChatDatabase\n",
        "# from metrics import MetricsCalculator\n",
        "# from config import DATABASE_FILE, THEME_COLOR, ENABLE_DARK_MODE, STREAM_OUTPUT, ENABLE_CHAT_EXPORT\n",
        "\n",
        "# # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–\n",
        "# if 'session_id' not in st.session_state:\n",
        "#    st.session_state.session_id = str(uuid.uuid4())\n",
        "# if 'chat_history' not in st.session_state:\n",
        "#    st.session_state.chat_history = []\n",
        "# if 'llm_generator' not in st.session_state:\n",
        "#    st.session_state.llm_generator = LLMGenerator()\n",
        "# if 'database' not in st.session_state:\n",
        "#    st.session_state.database = ChatDatabase(DATABASE_FILE)\n",
        "# if 'metrics_calculator' not in st.session_state:\n",
        "#    st.session_state.metrics_calculator = MetricsCalculator()\n",
        "\n",
        "# # ãƒšãƒ¼ã‚¸è¨­å®š\n",
        "# st.set_page_config(\n",
        "#    page_title=\"LLM Chat App\",\n",
        "#    page_icon=\"ğŸ¤–\",\n",
        "#    layout=\"wide\",\n",
        "#    initial_sidebar_state=\"expanded\",\n",
        "# )\n",
        "\n",
        "# # ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã®é©ç”¨\n",
        "# if ENABLE_DARK_MODE:\n",
        "#    st.markdown(\"\"\"\n",
        "#    <style>\n",
        "#    .main {background-color: #0E1117; color: white;}\n",
        "#    .sidebar .sidebar-content {background-color: #262730; color: white;}\n",
        "#    .stButton button {background-color: #4CAF50; color: white;}\n",
        "#    .stTextInput, .stTextArea {background-color: #262730; color: white;}\n",
        "#    </style>\n",
        "#    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä½œæˆ\n",
        "# with st.sidebar:\n",
        "#    st.title(\"LLM Chat App\")\n",
        "\n",
        "#    # ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠ\n",
        "#    page = st.radio(\n",
        "#        \"ãƒ¡ãƒ‹ãƒ¥ãƒ¼\",\n",
        "#        [\"ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ\", \"ğŸ“š å±¥æ­´\", \"ğŸ“Š åˆ†æ\"]\n",
        "#    )\n",
        "\n",
        "#    # ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
        "#    if page == \"ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ\":\n",
        "#        st.subheader(\"ãƒ¢ãƒ‡ãƒ«è¨­å®š\")\n",
        "\n",
        "#        # æ¸©åº¦ã®èª¿æ•´\n",
        "#        temperature = st.slider(\n",
        "#            \"æ¸©åº¦ (å‰µé€ æ€§)\",\n",
        "#            min_value=0.1,\n",
        "#            max_value=1.0,\n",
        "#            value=0.7,\n",
        "#            step=0.1,\n",
        "#            help=\"å€¤ãŒé«˜ã„ã»ã©å‰µé€ çš„ã§å¤šæ§˜ãªå¿œç­”ã«ãªã‚Šã¾ã™ã€‚ä½ã„ã¨æ±ºå®šè«–çš„ãªå¿œç­”ã«ãªã‚Šã¾ã™ã€‚\"\n",
        "#        )\n",
        "\n",
        "#        # æœ€å¤§é•·ã®èª¿æ•´\n",
        "#        max_length = st.slider(\n",
        "#            \"æœ€å¤§å¿œç­”é•·\",\n",
        "#            min_value=64,\n",
        "#            max_value=1024,\n",
        "#            value=512,\n",
        "#            step=64,\n",
        "#            help=\"ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\"\n",
        "#        )\n",
        "\n",
        "#        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒªã‚»ãƒƒãƒˆ\n",
        "#        if st.button(\"æ–°ã—ã„ä¼šè©±ã‚’é–‹å§‹\"):\n",
        "#            st.session_state.chat_history = []\n",
        "#            st.session_state.session_id = str(uuid.uuid4())\n",
        "#            st.success(\"æ–°ã—ã„ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã—ãŸ\")\n",
        "\n",
        "# # ãƒãƒ£ãƒƒãƒˆãƒšãƒ¼ã‚¸\n",
        "# if page == \"ğŸ’¬ ãƒãƒ£ãƒƒãƒˆ\":\n",
        "#    st.title(\"LLMãƒãƒ£ãƒƒãƒˆ\")\n",
        "\n",
        "#    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º\n",
        "#    for chat in st.session_state.chat_history:\n",
        "#        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
        "#        with st.chat_message(\"user\"):\n",
        "#            st.write(chat[\"user_input\"])\n",
        "\n",
        "#        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
        "#        with st.chat_message(\"assistant\"):\n",
        "#            st.write(chat[\"model_response\"])\n",
        "\n",
        "#            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤ºï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ï¼‰\n",
        "#            if \"metrics\" in chat and chat[\"metrics\"]:\n",
        "#                with st.expander(\"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™\"):\n",
        "#                    metrics = chat[\"metrics\"]\n",
        "\n",
        "#                    # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
        "#                    cols = st.columns(3)\n",
        "#                    with cols[0]:\n",
        "#                        st.metric(\"å¿œç­”æ™‚é–“\", f\"{metrics['response_time']:.2f}ç§’\")\n",
        "#                    with cols[1]:\n",
        "#                        st.metric(\"ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦\", f\"{metrics.get('token_generation_speed', 0):.1f} t/s\")\n",
        "#                    with cols[2]:\n",
        "#                        sentiment = metrics.get('sentiment', {}).get('vader_compound', 0)\n",
        "#                        st.metric(\"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢\", f\"{sentiment:.2f}\", delta=sentiment)\n",
        "\n",
        "#                    # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
        "#                    if 'sentiment' in metrics:\n",
        "#                        sentiment = metrics['sentiment']\n",
        "#                        st.write(\"**æ„Ÿæƒ…åˆ†æ:**\")\n",
        "#                        sentiment_df = pd.DataFrame({\n",
        "#                            'æŒ‡æ¨™': ['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ä¸­ç«‹', 'ä¸»è¦³æ€§'],\n",
        "#                            'å€¤': [\n",
        "#                                sentiment.get('vader_positive', 0),\n",
        "#                                sentiment.get('vader_negative', 0),\n",
        "#                                sentiment.get('vader_neutral', 0),\n",
        "#                                sentiment.get('subjectivity', 0)\n",
        "#                            ]\n",
        "#                        })\n",
        "#                        st.bar_chart(sentiment_df.set_index('æŒ‡æ¨™'))\n",
        "\n",
        "#            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯\n",
        "#            if not chat.get(\"feedback_rating\"):\n",
        "#                col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#                with col3:\n",
        "#                    st.write(\"ã“ã®å›ç­”ã¯ã„ã‹ãŒã§ã—ãŸã‹ï¼Ÿ\")\n",
        "\n",
        "#                col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#                with col1:\n",
        "#                    if st.button(\"ğŸ‘ æ‚ªã„\", key=f\"bad_{chat['id']}\"):\n",
        "#                        st.session_state.database.save_feedback(chat[\"id\"], 1)\n",
        "#                        st.experimental_rerun()\n",
        "#                with col2:\n",
        "#                    if st.button(\"ğŸ‘ æ™®é€š\", key=f\"fair_{chat['id']}\"):\n",
        "#                        st.session_state.database.save_feedback(chat[\"id\"], 3)\n",
        "#                        st.experimental_rerun()\n",
        "#                with col3:\n",
        "#                    if st.button(\"ğŸ‘ğŸ‘ è‰¯ã„\", key=f\"good_{chat['id']}\"):\n",
        "#                        st.session_state.database.save_feedback(chat[\"id\"], 5)\n",
        "#                        st.experimental_rerun()\n",
        "#            else:\n",
        "#                st.success(f\"è©•ä¾¡: {'ğŸ‘' * (chat['feedback_rating'] // 2)}\")\n",
        "\n",
        "#    # æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…¥åŠ›\n",
        "#    user_input = st.chat_input(\"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\")\n",
        "\n",
        "#    if user_input:\n",
        "#        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º\n",
        "#        with st.chat_message(\"user\"):\n",
        "#            st.write(user_input)\n",
        "\n",
        "#        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º\n",
        "#        with st.chat_message(\"assistant\"):\n",
        "#            message_placeholder = st.empty()\n",
        "\n",
        "#            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã®ãƒãƒ³ãƒ‰ãƒ©\n",
        "#            if STREAM_OUTPUT:\n",
        "#                full_response = \"\"\n",
        "\n",
        "#                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
        "#                def stream_handler(new_text):\n",
        "#                    nonlocal full_response\n",
        "#                    full_response += new_text\n",
        "#                    message_placeholder.markdown(full_response + \"â–Œ\")\n",
        "\n",
        "#                # å¿œç­”ç”Ÿæˆ\n",
        "#                start_time = time.time()\n",
        "#                response, gen_time = st.session_state.llm_generator.generate_text(\n",
        "#                    user_input,\n",
        "#                    max_length=max_length,\n",
        "#                    temperature=temperature,\n",
        "#                    stream_handler=stream_handler\n",
        "#                )\n",
        "\n",
        "#                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã‚’æœ€çµ‚æ›´æ–°\n",
        "#                message_placeholder.markdown(full_response)\n",
        "#            else:\n",
        "#                # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›\n",
        "#                start_time = time.time()\n",
        "#                with st.spinner(\"è€ƒãˆä¸­...\"):\n",
        "#                    response, gen_time = st.session_state.llm_generator.generate_text(\n",
        "#                        user_input,\n",
        "#                        max_length=max_length,\n",
        "#                        temperature=temperature\n",
        "#                    )\n",
        "#                message_placeholder.markdown(response)\n",
        "\n",
        "#            # ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã®è¨ˆç®—\n",
        "#            metrics = st.session_state.metrics_calculator.calculate_all_metrics(\n",
        "#                user_input, response, gen_time\n",
        "#            )\n",
        "\n",
        "#            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜\n",
        "#            chat_id = st.session_state.database.save_chat(\n",
        "#                st.session_state.session_id,\n",
        "#                user_input,\n",
        "#                response,\n",
        "#                gen_time,\n",
        "#                metrics\n",
        "#            )\n",
        "\n",
        "#            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜\n",
        "#            st.session_state.chat_history.append({\n",
        "#                \"id\": chat_id,\n",
        "#                \"user_input\": user_input,\n",
        "#                \"model_response\": response,\n",
        "#                \"timestamp\": datetime.now().isoformat(),\n",
        "#                \"metrics\": metrics\n",
        "#            })\n",
        "\n",
        "#            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯UI\n",
        "#            with st.expander(\"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™\"):\n",
        "#                cols = st.columns(3)\n",
        "#                with cols[0]:\n",
        "#                    st.metric(\"å¿œç­”æ™‚é–“\", f\"{gen_time:.2f}ç§’\")\n",
        "#                with cols[1]:\n",
        "#                    st.metric(\"ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦\", f\"{metrics.get('token_generation_speed', 0):.1f} t/s\")\n",
        "#                with cols[2]:\n",
        "#                    sentiment = metrics.get('sentiment', {}).get('vader_compound', 0)\n",
        "#                    st.metric(\"æ„Ÿæƒ…ã‚¹ã‚³ã‚¢\", f\"{sentiment:.2f}\", delta=sentiment)\n",
        "\n",
        "#                # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
        "#                if 'sentiment' in metrics:\n",
        "#                    sentiment = metrics['sentiment']\n",
        "#                    st.write(\"**æ„Ÿæƒ…åˆ†æ:**\")\n",
        "#                    sentiment_df = pd.DataFrame({\n",
        "#                        'æŒ‡æ¨™': ['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ä¸­ç«‹', 'ä¸»è¦³æ€§'],\n",
        "#                        'å€¤': [\n",
        "#                            sentiment.get('vader_positive', 0),\n",
        "#                            sentiment.get('vader_negative', 0),\n",
        "#                            sentiment.get('vader_neutral', 0),\n",
        "#                            sentiment.get('subjectivity', 0)\n",
        "#                        ]\n",
        "#                    })\n",
        "#                    st.bar_chart(sentiment_df.set_index('æŒ‡æ¨™'))\n",
        "\n",
        "#            col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#            with col3:\n",
        "#                st.write(\"ã“ã®å›ç­”ã¯ã„ã‹ãŒã§ã—ãŸã‹ï¼Ÿ\")\n",
        "\n",
        "#            col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#            with col1:\n",
        "#                if st.button(\"ğŸ‘ æ‚ªã„\", key=f\"bad_{chat_id}\"):\n",
        "#                    st.session_state.database.save_feedback(chat_id, 1)\n",
        "#                    st.experimental_rerun()\n",
        "#            with col2:\n",
        "#                if st.button(\"ğŸ‘ æ™®é€š\", key=f\"fair_{chat_id}\"):\n",
        "#                    st.session_state.database.save_feedback(chat_id, 3)\n",
        "#                    st.experimental_rerun()\n",
        "#            with col3:\n",
        "#                if st.button(\"ğŸ‘ğŸ‘ è‰¯ã„\", key=f\"good_{chat_id}\"):\n",
        "#                    st.session_state.database.save_feedback(chat_id, 5)\n",
        "#                    st.experimental_rerun()\n",
        "\n",
        "# # å±¥æ­´ãƒšãƒ¼ã‚¸\n",
        "# elif page == \"ğŸ“š å±¥æ­´\":\n",
        "#    st.title(\"ãƒãƒ£ãƒƒãƒˆå±¥æ­´\")\n",
        "\n",
        "#    # å±¥æ­´ã®è¡¨ç¤º\n",
        "#    all_history = st.checkbox(\"ã™ã¹ã¦ã®å±¥æ­´ã‚’è¡¨ç¤º\", value=False)\n",
        "\n",
        "#    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³\n",
        "#    total_chats = st.session_state.database.get_total_chat_count(\n",
        "#        None if all_history else st.session_state.session_id\n",
        "#    )\n",
        "\n",
        "#    items_per_page = 5\n",
        "#    total_pages = (total_chats + items_per_page - 1) // items_per_page\n",
        "\n",
        "#    col1, col2, col3 = st.columns([1, 3, 1])\n",
        "#    with col2:\n",
        "#        page_number = st.slider(\"ãƒšãƒ¼ã‚¸\", 1, max(1, total_pages), 1)\n",
        "\n",
        "#    offset = (page_number - 1) * items_per_page\n",
        "\n",
        "#    history = st.session_state.database.get_chat_history(\n",
        "#        None if all_history else st.session_state.session_id,\n",
        "#        limit=items_per_page,\n",
        "#        offset=offset\n",
        "#    )\n",
        "\n",
        "#    if not history:\n",
        "#        st.info(\"å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒãƒ£ãƒƒãƒˆã‚’å§‹ã‚ã¾ã—ã‚‡ã†ï¼\")\n",
        "#    else:\n",
        "#        # CSV/JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½\n",
        "#        if ENABLE_CHAT_EXPORT:\n",
        "#            col1, col2 = st.columns(2)\n",
        "\n",
        "#            with col1:\n",
        "#                # CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n",
        "#                csv_data = io.StringIO()\n",
        "#                history_df = pd.DataFrame([{\n",
        "#                    'timestamp': item['timestamp'],\n",
        "#                    'user_input': item['user_input'],\n",
        "#                    'model_response': item['model_response'],\n",
        "#                    'response_time': item['response_time'],\n",
        "#                    'rating': item['feedback_rating'] or 0\n",
        "#                } for item in history])\n",
        "\n",
        "#                history_df.to_csv(csv_data, index=False)\n",
        "\n",
        "#                st.download_button(\n",
        "#                    label=\"CSVã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\",\n",
        "#                    data=csv_data.getvalue(),\n",
        "#                    file_name=f\"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
        "#                    mime=\"text/csv\"\n",
        "#                )\n",
        "\n",
        "#            with col2:\n",
        "#                # JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n",
        "#                json_data = json.dumps([{\n",
        "#                    'timestamp': item['timestamp'],\n",
        "#                    'user_input': item['user_input'],\n",
        "#                    'model_response': item['model_response'],\n",
        "#                    'response_time': item['response_time'],\n",
        "#                    'metrics': item['metrics'],\n",
        "#                    'rating': item['feedback_rating']\n",
        "#                } for item in history], indent=2)\n",
        "\n",
        "#                st.download_button(\n",
        "#                    label=\"JSONã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\",\n",
        "#                    data=json_data,\n",
        "#                    file_name=f\"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n",
        "#                    mime=\"application/json\"\n",
        "#                )\n",
        "\n",
        "#        # å±¥æ­´è¡¨ç¤º\n",
        "#        for item in history:\n",
        "#            with st.expander(f\"{item['timestamp'][:19]} - {item['user_input'][:50]}...\"):\n",
        "#                st.subheader(\"ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›\")\n",
        "#                st.write(item['user_input'])\n",
        "\n",
        "#                st.subheader(\"ãƒ¢ãƒ‡ãƒ«å¿œç­”\")\n",
        "#                st.write(item['model_response'])\n",
        "\n",
        "#                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º\n",
        "#                col1, col2, col3 = st.columns(3)\n",
        "\n",
        "#                with col1:\n",
        "#                    st.metric(\"å¿œç­”æ™‚é–“\", f\"{item['response_time']:.2f}ç§’\")\n",
        "\n",
        "#                with col2:\n",
        "#                    if 'metrics' in item and item['metrics']:\n",
        "#                        metrics = item['metrics']\n",
        "#                        if 'token_generation_speed' in metrics:\n",
        "#                            st.metric(\"ç”Ÿæˆé€Ÿåº¦\", f\"{metrics['token_generation_speed']:.1f} t/s\")\n",
        "\n",
        "#                with col3:\n",
        "#                    if item['feedback_rating']:\n",
        "#                        st.metric(\"è©•ä¾¡\", f\"{item['feedback_rating']}/5\")\n",
        "#                    else:\n",
        "#                        st.info(\"è©•ä¾¡ãªã—\")\n",
        "\n",
        "#                # æ„Ÿæƒ…åˆ†æè¡¨ç¤º\n",
        "#                if 'metrics' in item and item['metrics'] and 'sentiment' in item['metrics']:\n",
        "#                    sentiment = item['metrics']['sentiment']\n",
        "#                    st.subheader(\"æ„Ÿæƒ…åˆ†æ\")\n",
        "\n",
        "#                    sentiment_df = pd.DataFrame({\n",
        "#                        'æŒ‡æ¨™': ['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ä¸­ç«‹', 'ä¸»è¦³æ€§'],\n",
        "#                        'å€¤': [\n",
        "#                            sentiment.get('vader_positive', 0),\n",
        "#                            sentiment.get('vader_negative', 0),\n",
        "#                            sentiment.get('vader_neutral', 0),\n",
        "#                            sentiment.get('subjectivity', 0)\n",
        "#                        ]\n",
        "#                    })\n",
        "\n",
        "#                    st.bar_chart(sentiment_df.set_index('æŒ‡æ¨™'))\n",
        "\n",
        "# # åˆ†æãƒšãƒ¼ã‚¸\n",
        "# elif page == \"ğŸ“Š åˆ†æ\":\n",
        "#    st.title(\"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ\")\n",
        "\n",
        "#    # çµ±è¨ˆæƒ…å ±ã®å–å¾—\n",
        "#    stats = st.session_state.database.get_statistics()\n",
        "\n",
        "#    # åŸºæœ¬çµ±è¨ˆ\n",
        "#    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "#    with col1:\n",
        "#        st.metric(\"ç·ãƒãƒ£ãƒƒãƒˆæ•°\", stats['total_chats'])\n",
        "\n",
        "#    with col2:\n",
        "#        st.metric(\"å¹³å‡å¿œç­”æ™‚é–“\", f\"{stats['avg_response_time']:.2f}ç§’\")\n",
        "\n",
        "#    with col3:\n",
        "#        st.metric(\"å¹³å‡è©•ä¾¡\", f\"{stats['avg_rating']:.1f}/5\")\n",
        "\n",
        "#    # è©•ä¾¡åˆ†å¸ƒ\n",
        "#    st.subheader(\"è©•ä¾¡åˆ†å¸ƒ\")\n",
        "\n",
        "#    rating_df = pd.DataFrame({\n",
        "#        'è©•ä¾¡': list(stats['rating_distribution'].keys()),\n",
        "#        'æ•°': list(stats['rating_distribution'].values())\n",
        "#    })\n",
        "\n",
        "#    if not rating_df.empty:\n",
        "#        fig, ax = plt.subplots()\n",
        "#        ax.bar(rating_df['è©•ä¾¡'], rating_df['æ•°'])\n",
        "#        ax.set_xlabel('è©•ä¾¡ç‚¹æ•°')\n",
        "#        ax.set_ylabel('å›æ•°')\n",
        "#        ax.set_xticks(range(1, 6))\n",
        "#        ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "#        st.pyplot(fig)\n",
        "#    else:\n",
        "#        st.info(\"ã¾ã è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "#    # æ™‚ç³»åˆ—åˆ†æ\n",
        "#    st.subheader(\"å¿œç­”æ™‚é–“ã®æ¨ç§»\")\n",
        "\n",
        "#    # ç›´è¿‘20ä»¶ã®ãƒãƒ£ãƒƒãƒˆå–å¾—\n",
        "#    history = st.session_state.database.get_chat_history(limit=20)\n",
        "\n",
        "#    if history:\n",
        "#        time_df = pd.DataFrame([{\n",
        "#            'timestamp': item['timestamp'],\n",
        "#            'response_time': item['response_time']\n",
        "#        } for item in history])\n",
        "\n",
        "#        time_df['timestamp'] = pd.to_datetime(time_df['timestamp'])\n",
        "#        time_df = time_df.sort_values(by='timestamp')\n",
        "\n",
        "#        st.line_chart(time_df.set_index('timestamp'))\n",
        "#    else:\n",
        "#        st.info(\"æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "#    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æŒ‡æ¨™ã®æ¯”è¼ƒ\n",
        "#    st.subheader(\"æ„Ÿæƒ…åˆ†æ\")\n",
        "\n",
        "#    # æ„Ÿæƒ…åˆ†æã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
        "#    sentiment_data = []\n",
        "\n",
        "#    for item in history:\n",
        "#        if 'metrics' in item and item['metrics'] and 'sentiment' in item['metrics']:\n",
        "#            sentiment = item['metrics']['sentiment']\n",
        "#            sentiment_data.append({\n",
        "#                'timestamp': item['timestamp'],\n",
        "#                'positive': sentiment.get('vader_positive', 0),\n",
        "#                'negative': sentiment.get('vader_negative', 0),\n",
        "#                'neutral': sentiment.get('vader_neutral', 0)\n",
        "#            })\n",
        "\n",
        "#    if sentiment_data:\n",
        "#        sentiment_df = pd.DataFrame(sentiment_data)\n",
        "#        sentiment_df['timestamp'] = pd.to_datetime(sentiment_df['timestamp'])\n",
        "#        sentiment_df = sentiment_df.sort_values(by='timestamp')\n",
        "\n",
        "#        # ç©ã¿ä¸Šã’ã‚°ãƒ©ãƒ•ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›\n",
        "#        chart_data = pd.DataFrame({\n",
        "#            'Positive': sentiment_df['positive'],\n",
        "#            'Negative': sentiment_df['negative'],\n",
        "#            'Neutral': sentiment_df['neutral']\n",
        "#        }, index=sentiment_df['timestamp'])\n",
        "\n",
        "#        st.area_chart(chart_data)\n",
        "#    else:\n",
        "#        st.info(\"æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "id": "_vh7DCt87ac7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TBQyTTWTELSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee7e8b9-33b9-4ef5-fea0-b7386a596c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å…¬é–‹URL: https://a466-34-68-20-49.ngrok-free.app\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.68.20.49:8501\u001b[0m\n",
            "\u001b[0m\n",
            "NLTK loaded successfully.\n",
            "2025-04-20 06:02:45.928364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745128966.207966    7139 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745128966.283511    7139 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-20 06:02:46.875317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "config.json: 100% 805/805 [00:00<00:00, 4.19MB/s]\n",
            "model.safetensors.index.json: 100% 24.2k/24.2k [00:00<00:00, 44.9MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/241M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   4% 10.5M/241M [00:00<00:02, 82.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.99G [00:00<01:31, 54.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.99G [00:00<01:06, 75.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  13% 31.5M/241M [00:00<00:01, 114MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.99G [00:00<00:40, 121MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  22% 52.4M/241M [00:00<00:01, 117MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.99G [00:00<00:33, 147MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  30% 73.4M/241M [00:00<00:01, 115MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.99G [00:00<00:37, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.99G [00:00<00:37, 129MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  39% 94.4M/241M [00:00<00:01, 116MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.99G [00:01<00:35, 138MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  48% 115M/241M [00:00<00:01, 118MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.99G [00:01<00:32, 148MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  57% 136M/241M [00:01<00:00, 119MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.99G [00:01<00:31, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.99G [00:01<00:29, 163MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  65% 157M/241M [00:01<00:00, 120MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.99G [00:01<00:29, 161MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  74% 178M/241M [00:01<00:00, 116MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.99G [00:01<00:31, 149MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  83% 199M/241M [00:01<00:00, 119MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.99G [00:01<00:30, 153MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  91% 220M/241M [00:01<00:00, 130MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.99G [00:01<00:30, 153MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 241M/241M [00:02<00:00, 118MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.99G [00:02<00:32, 144MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.99G [00:02<00:27, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.99G [00:02<00:26, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.99G [00:02<00:25, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.99G [00:02<00:24, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.99G [00:02<00:23, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.99G [00:02<00:23, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.99G [00:02<00:23, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.99G [00:03<00:22, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.99G [00:03<00:22, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.99G [00:03<00:22, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.99G [00:03<00:23, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.99G [00:03<00:23, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.99G [00:03<00:23, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.99G [00:03<00:23, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.99G [00:03<00:23, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.99G [00:03<00:22, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.99G [00:04<00:22, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.99G [00:04<00:22, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.99G [00:04<00:21, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.99G [00:04<00:21, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.99G [00:04<00:20, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.99G [00:04<00:20, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.99G [00:04<00:20, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.99G [00:04<00:19, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 870M/4.99G [00:04<00:20, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.99G [00:05<00:19, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 923M/4.99G [00:05<00:19, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.99G [00:05<00:18, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.99G [00:05<00:19, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/4.99G [00:05<00:18, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.99G [00:05<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.99G [00:05<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.99G [00:06<00:18, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.99G [00:06<00:18, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.99G [00:06<00:17, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.99G [00:06<00:17, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.99G [00:06<00:18, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.99G [00:06<00:18, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.99G [00:06<00:18, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.99G [00:07<00:17, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.99G [00:07<00:17, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.99G [00:07<00:17, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.99G [00:07<00:17, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.99G [00:07<00:17, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.99G [00:07<00:17, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.99G [00:07<00:18, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.99G [00:07<00:19, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.99G [00:08<00:19, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.52G/4.99G [00:08<00:19, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.99G [00:08<00:20, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.99G [00:08<00:21, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.99G [00:08<00:21, 161MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.99G [00:08<00:21, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.99G [00:09<00:26, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.99G [00:09<00:39, 84.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.67G/4.99G [00:09<00:41, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.99G [00:09<00:41, 80.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.99G [00:10<00:39, 83.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.72G/4.99G [00:10<00:39, 82.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.99G [00:10<00:38, 83.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.99G [00:10<00:39, 83.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.99G [00:10<00:39, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.99G [00:11<00:50, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.99G [00:11<00:52, 61.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.99G [00:11<00:46, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.99G [00:11<00:49, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.99G [00:11<00:59, 53.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.99G [00:11<00:59, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.99G [00:12<01:00, 52.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.99G [00:12<00:51, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.99G [00:12<00:55, 56.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.99G [00:12<00:57, 54.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.99G [00:12<00:43, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.99G [00:13<00:34, 88.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.99G [00:13<00:48, 63.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.99G [00:13<00:38, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.99G [00:13<00:36, 82.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.99G [00:13<00:31, 95.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.99G [00:13<00:28, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.99G [00:14<00:29, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.99G [00:14<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.99G [00:14<00:27, 107MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.07G/4.99G [00:14<00:25, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.99G [00:14<00:23, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.99G [00:15<00:29, 98.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.99G [00:15<00:27, 105MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.99G [00:15<00:27, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.99G [00:15<00:25, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.99G [00:15<00:23, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.99G [00:16<00:22, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.99G [00:16<00:21, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.99G [00:16<00:21, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.99G [00:16<00:21, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.99G [00:16<00:20, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.32G/4.99G [00:16<00:20, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.99G [00:16<00:20, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.99G [00:19<01:40, 26.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.99G [00:19<01:26, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.99G [00:19<01:04, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.41G/4.99G [00:19<00:48, 52.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.99G [00:19<00:39, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.99G [00:19<00:32, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.99G [00:20<00:27, 90.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.99G [00:20<00:25, 99.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.52G/4.99G [00:20<00:23, 107MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.99G [00:20<00:22, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.99G [00:20<00:21, 112MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.99G [00:20<00:20, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.99G [00:21<00:20, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.99G [00:21<00:19, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.99G [00:21<00:18, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.66G/4.99G [00:21<00:18, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.99G [00:21<00:18, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.99G [00:22<00:19, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.99G [00:22<00:19, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.99G [00:22<00:19, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.77G/4.99G [00:22<00:19, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.99G [00:22<00:18, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.81G/4.99G [00:25<01:37, 22.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.99G [00:25<01:12, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.99G [00:25<00:57, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.86G/4.99G [00:26<00:54, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.99G [00:26<00:40, 51.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.99G [00:26<00:32, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.99G [00:26<00:27, 74.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.99G [00:26<00:27, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.97G/4.99G [00:27<00:23, 85.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.99G [00:27<00:27, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.01G/4.99G [00:27<00:24, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.99G [00:27<00:21, 91.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.99G [00:27<00:18, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.99G [00:28<00:17, 111MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.99G [00:28<00:16, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/4.99G [00:28<00:16, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.99G [00:28<00:15, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.99G [00:28<00:14, 123MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.99G [00:28<00:14, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.99G [00:29<00:25, 69.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.99G [00:29<00:21, 82.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.99G [00:29<00:18, 92.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.26G/4.99G [00:29<00:16, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.99G [00:30<00:15, 111MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.99G [00:30<00:14, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.99G [00:30<00:14, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.99G [00:30<00:13, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.37G/4.99G [00:30<00:13, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.99G [00:30<00:12, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.41G/4.99G [00:31<00:12, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.99G [00:31<00:11, 133MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.99G [00:31<00:11, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.99G [00:31<00:11, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.99G [00:31<00:11, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.51G/4.99G [00:31<00:11, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.99G [00:32<00:11, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.99G [00:34<01:02, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.57G/4.99G [00:35<01:13, 19.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.99G [00:35<00:51, 27.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.61G/4.99G [00:35<00:37, 36.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.99G [00:36<00:28, 47.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.99G [00:36<00:22, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.99G [00:36<00:18, 71.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.99G [00:36<00:15, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.71G/4.99G [00:36<00:13, 92.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.99G [00:36<00:12, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.99G [00:37<00:11, 107MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.99G [00:37<00:10, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.99G [00:37<00:09, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.99G [00:37<00:09, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.99G [00:37<00:09, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.86G/4.99G [00:37<00:09, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.99G [00:38<00:11, 95.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.99G [00:38<00:10, 105MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.99G [00:38<00:09, 112MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.99G [00:39<00:19, 52.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.95G/4.99G [00:39<00:18, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.99G [00:39<00:14, 70.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.99G [00:39<00:11, 84.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.99G [00:39<00:10, 96.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.99G [00:40<00:08, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.06G/4.99G [00:40<00:08, 113MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.99G [00:40<00:07, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.99G [00:40<00:07, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.99G [00:40<00:06, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.99G [00:40<00:06, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.16G/4.99G [00:41<00:06, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.99G [00:41<00:06, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.20G/4.99G [00:41<00:06, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.99G [00:41<00:05, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.99G [00:41<00:05, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.99G [00:41<00:05, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.99G [00:42<00:05, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.31G/4.99G [00:45<00:40, 16.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.99G [00:46<00:29, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.35G/4.99G [00:46<00:21, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.99G [00:46<00:15, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.99G [00:46<00:12, 49.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.99G [00:46<00:09, 60.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.99G [00:46<00:07, 71.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.46G/4.99G [00:47<00:08, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.99G [00:47<00:06, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.99G [00:47<00:05, 83.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.99G [00:47<00:04, 94.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.99G [00:47<00:04, 102MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.56G/4.99G [00:48<00:03, 108MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.99G [00:48<00:03, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.60G/4.99G [00:48<00:03, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.99G [00:48<00:02, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.99G [00:48<00:02, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.99G [00:52<00:17, 18.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.99G [00:52<00:11, 25.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.71G/4.99G [00:52<00:08, 33.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.99G [00:52<00:05, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.75G/4.99G [00:52<00:04, 55.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.99G [00:52<00:03, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.99G [00:53<00:02, 75.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.81G/4.99G [00:53<00:02, 86.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.99G [00:53<00:01, 96.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.85G/4.99G [00:53<00:01, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.99G [00:53<00:00, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.99G [00:53<00:00, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.99G [00:54<00:00, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.99G [00:54<00:00, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.96G/4.99G [00:54<00:00, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:54<00:00, 91.3MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:54<00:00, 27.46s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 10.58it/s]\n",
            "generation_config.json: 100% 168/168 [00:00<00:00, 1.06MB/s]\n",
            "tokenizer_config.json: 100% 46.9k/46.9k [00:00<00:00, 14.4MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 43.1MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 147MB/s]\n",
            "special_tokens_map.json: 100% 555/555 [00:00<00:00, 3.55MB/s]\n",
            "Device set to use cpu\n",
            "2025-04-20 06:04:08.189 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"å…¬é–‹URL: {public_url}\")\n",
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxliKOTawSDe"
      },
      "source": [
        "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ©Ÿèƒ½ã¨ã—ã¦ã¯ã€ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚„å±¥æ­´é–²è¦§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã¯ã€Streamlitã«ã‚ˆã‚‹UIéƒ¨åˆ†ã ã‘ã§ã¯ãªãã€SQLiteã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ä¿å­˜ã‚„LLMã®ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—ãŸæ¨è«–ãªã©ã®å‡¦ç†ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- **`app.py`**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã€å±¥æ­´é–²è¦§ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®UIã‚’æä¾›ã—ã¾ã™ã€‚\n",
        "- **`ui.py`**: ãƒãƒ£ãƒƒãƒˆãƒšãƒ¼ã‚¸ã‚„å±¥æ­´é–²è¦§ãƒšãƒ¼ã‚¸ãªã©ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®UIãƒ­ã‚¸ãƒƒã‚¯ã‚’ç®¡ç†ã—ã¾ã™ã€‚\n",
        "- **`llm.py`**: LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚\n",
        "- **`database.py`**: SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¿å­˜ãƒ»ç®¡ç†ã—ã¾ã™ã€‚\n",
        "- **`metrics.py`**: BLEUã‚¹ã‚³ã‚¢ã‚„ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãªã©ã€å›ç­”ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚\n",
        "- **`data.py`**: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã‚„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ã‚’è¡Œã†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚\n",
        "- **`config.py`**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«åã‚„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã‚’ç®¡ç†ã—ã¾ã™ã€‚\n",
        "- **`requirements.txt`**: ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvm8sWFPELSP"
      },
      "source": [
        "å¾Œç‰‡ä»˜ã‘ã¨ã—ã¦ã€ä½¿ã†å¿…è¦ã®ãªã„ngrokã®ãƒˆãƒ³ãƒãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WFJC2TmZELSP"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUXhIzV7ELSP"
      },
      "source": [
        "# 03_FastAPI\n",
        "\n",
        "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€Œ03_FastAPIã€ã«ç§»å‹•ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ejjDLxr3kfC"
      },
      "outputs": [],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/03_FastAPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45TDsNzELSQ"
      },
      "source": [
        "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uv6glCz5a7Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfrmE2VmELSQ"
      },
      "source": [
        "ngrokã¨huggigfaceã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€èªè¨¼ã‚’è¡Œã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELzWhMFORRIO"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wztc2CELSQ"
      },
      "source": [
        "ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã™ã€‚\n",
        "\n",
        "ã€Œ02_streamlit_appã€ã‹ã‚‰ç¶šã‘ã¦ã€Œ03_FastAPIã€ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒæ¸ˆã‚“ã§ã„ã‚‹ãŸã‚ã€ã™ãã«ã‚µãƒ¼ãƒ“ã‚¹ãŒç«‹ã¡ä¸ŠãŒã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã€Œ03_FastAPIã€ã®ã¿ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹å ´åˆã¯ã€åˆå›ã®èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå§‹ã¾ã‚‹ã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒçµ‚ã‚ã‚‹ã¾ã§æ•°åˆ†é–“å¾…ã¡ã¾ã—ã‚‡ã†ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meQ4SwISn3IQ"
      },
      "outputs": [],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLubjIhbELSR"
      },
      "source": [
        "FastAPIãŒèµ·å‹•ã™ã‚‹ã¨ã€APIã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒé€šä¿¡ã™ã‚‹ãŸã‚ã®URLï¼ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰ãŒä½œã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "URLãŒä½œã‚‰ã‚Œã‚‹ã®ã¨åˆã‚ã›ã¦ã€Swagger UIã¨ã„ã†Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒä½œã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "Swagger UIã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã§ã€APIã®ä»•æ§˜ã‚’ç¢ºèªã§ããŸã‚Šã€APIã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "\n",
        "Swagger UIã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€APIã‚’é€šã—ã¦LLMã‚’å‹•ã‹ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgumW3mGELSR"
      },
      "source": [
        "å¾Œç‰‡ä»˜ã‘ã¨ã—ã¦ã€ä½¿ã†å¿…è¦ã®ãªã„ngrokã®ãƒˆãƒ³ãƒãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJymTZio-WPJ"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}