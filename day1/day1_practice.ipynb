{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSpnWBP5ELSI"
      },
      "source": [
        "# 実践演習 Day 1：streamlitとFastAPIのデモ\n",
        "このノートブックでは以下の内容を学習します。\n",
        "\n",
        "- 必要なライブラリのインストールと環境設定\n",
        "- Hugging Faceからモデルを用いたStreamlitのデモアプリ\n",
        "- FastAPIとngrokを使用したAPIの公開方法\n",
        "\n",
        "演習を始める前に、HuggingFaceとngrokのアカウントを作成し、\n",
        "それぞれのAPIトークンを取得する必要があります。\n",
        "\n",
        "\n",
        "演習の時間では、以下の3つのディレクトリを順に説明します。\n",
        "\n",
        "1. 01_streamlit_UI\n",
        "2. 02_streamlit_app\n",
        "3. 03_FastAPI\n",
        "\n",
        "2つ目や3つ目からでも始められる様にノートブックを作成しています。\n",
        "\n",
        "復習の際にもこのノートブックを役立てていただければと思います。\n",
        "\n",
        "### 注意事項\n",
        "「02_streamlit_app」と「03_FastAPI」では、GPUを使用します。\n",
        "\n",
        "これらを実行する際は、Google Colab画面上のメニューから「編集」→ 「ノートブックの設定」\n",
        "\n",
        "「ハードウェアアクセラレーター」の項目の中から、「T4 GPU」を選択してください。\n",
        "\n",
        "このノートブックのデフォルトは「CPU」になっています。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhtHkJOgELSL"
      },
      "source": [
        "# 環境変数の設定（1~3共有）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-FjBp4MMQHM"
      },
      "source": [
        "GitHubから演習用のコードをCloneします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AIXMavdDEP8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fafed51-a8f6-4147-bb25-43e65ac3df73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lecture-ai-engineering'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 41 (delta 7), reused 5 (delta 5), pack-reused 13 (from 1)\u001b[K\n",
            "Receiving objects: 100% (41/41), 34.04 KiB | 571.00 KiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC8n7yZ_vs1K"
      },
      "source": [
        "必要なAPIトークンを.envに設定します。\n",
        "\n",
        "「lecture-ai-engineering/day1」の配下に、「.env_template」ファイルが存在しています。\n",
        "\n",
        "隠しファイルのため表示されていない場合は、画面左側のある、目のアイコンの「隠しファイルの表示」ボタンを押してください。\n",
        "\n",
        "「.env_template」のファイル名を「.env」に変更します。「.env」ファイルを開くと、以下のような中身になっています。\n",
        "\n",
        "\n",
        "```\n",
        "HUGGINGFACE_TOKEN=\"hf-********\"\n",
        "NGROK_TOKEN=\"********\"\n",
        "```\n",
        "ダブルクオーテーションで囲まれた文字列をHuggingfaceのアクセストークンと、ngrokの認証トークンで書き変えてください。\n",
        "\n",
        "それぞれのアカウントが作成済みであれば、以下のURLからそれぞれのトークンを取得できます。\n",
        "\n",
        "- Huggingfaceのアクセストークン\n",
        "https://huggingface.co/docs/hub/security-tokens\n",
        "\n",
        "- ngrokの認証トークン\n",
        "https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "書き換えたら、「.env」ファイルをローカルのPCにダウンロードしてください。\n",
        "\n",
        "「01_streamlit_UI」から「02_streamlit_app」へ進む際に、CPUからGPUの利用に切り替えるため、セッションが一度切れてしまいます。\n",
        "\n",
        "その際に、トークンを設定した「.env」ファイルは再作成することになるので、その手間を減らすために「.env」ファイルをダウンロードしておくと良いです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py1BFS5RqcSS"
      },
      "source": [
        "「.env」ファイルを読み込み、環境変数として設定します。次のセルを実行し、最終的に「True」が表示されていればうまく読み込めています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bvEowFfg5lrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa8488d-aa6f-45b6-c7eb-0f06bd6a8165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "/content/lecture-ai-engineering/day1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "%cd /content/lecture-ai-engineering/day1\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "環境変数を一時的に保存しておくコード"
      ],
      "metadata": {
        "id": "fTE7nUlM160Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def save_env_backup():\n",
        "        env_backup = {\n",
        "                \"HUGGINGFACE_TOKEN\": os.environ.get(\"HUGGINGFACE_TOKEN\"),\n",
        "                \"NGROK_TOKEN\": os.environ.get(\"NGROK_TOKEN\")\n",
        "            }\n",
        "        with open('/content/env_backup.json', 'w') as f:\n",
        "              json.dump(env_backup, f)\n",
        "        print(\"環境変数のバックアップを作成しました。セッションが切断された場合は restore_env_backup() を実行してください。\")\n",
        "\n",
        "        # 環境変数を復元する関数\n",
        "        def restore_env_backup():\n",
        "            try:\n",
        "                with open('/content/env_backup.json', 'r') as f:\n",
        "                    env_backup = json.load(f)\n",
        "\n",
        "                # 環境変数を復元\n",
        "                for key, value in env_backup.items():\n",
        "                    os.environ[key] = value\n",
        "\n",
        "                # .env ファイルを再作成\\n\",\n",
        "                with open('/content/lecture-ai-engineering/day1/.env', 'w') as f:\n",
        "                    for key, value in env_backup.items():\n",
        "                        f.write(f\"{key}={value}\")\n",
        "\n",
        "                print(\"環境変数を復元しました。\")\n",
        "                return True\n",
        "            except FileNotFoundError:\n",
        "                print(\"バックアップファイルが見つかりません。環境変数を手動で設定してください。\")\n",
        "                return False\n",
        "\n",
        "        # バックアップを作成\n",
        "        save_env_backup()"
      ],
      "metadata": {
        "id": "BLkrs1sc2CTT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os0Yk6gaELSM"
      },
      "source": [
        "# 01_streamlit_UI\n",
        "\n",
        "ディレクトリ「01_streamlit_UI」に移動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S28XgOm0ELSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113a90ce-059f-4644-cd6e-f361dd533d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/01_streamlit_UI\n"
          ]
        }
      ],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/01_streamlit_UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVp-aEIkELSM"
      },
      "source": [
        "必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nBe41LFiELSN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 追加ライブラリのインストール\n",
        "!pip install streamlit-option-menu streamlit-authenticator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**新しい UI コンポーネントを追加**"
      ],
      "metadata": {
        "id": "tw3EGhkd4MZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile custom_ui.py\n",
        "\"\"\"\n",
        "custom_ui.py\n",
        "-----------------\n",
        "Streamlit 用のサイドバー＆カスタムページ部品。\n",
        "• メニュー選択・テーマ切替\n",
        "• ホームページのカードとプログレスバー\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import time\n",
        "import streamlit as st\n",
        "from streamlit_option_menu import option_menu\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# サイドバー\n",
        "# ----------------------------------------------------------------------\n",
        "_MENU_ITEMS = [\"ホーム\", \"基本要素\", \"レイアウト\", \"入力要素\", \"テーマ設定\"]\n",
        "_ICONS = [\"house\", \"list-task\", \"columns\", \"input-cursor\", \"palette\"]\n",
        "\n",
        "\n",
        "def _apply_dark_theme(enable_dark: bool) -> None:\n",
        "    \"\"\"ダークテーマを CSS で適用／解除する。\"\"\"\n",
        "    if enable_dark:\n",
        "        st.markdown(\n",
        "            \"\"\"\n",
        "            <style>\n",
        "            .main {background-color: #0E1117; color: #FFFFFF;}\n",
        "            .sidebar .sidebar-content {background-color: #262730; color: #FFFFFF;}\n",
        "            </style>\n",
        "            \"\"\",\n",
        "            unsafe_allow_html=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_sidebar() -> str:\n",
        "    \"\"\"\n",
        "    カスタムサイドバーを生成し、選択されたメニューを返す。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        現在選択中のページ名。\n",
        "    \"\"\"\n",
        "    with st.sidebar:\n",
        "        selected = option_menu(\n",
        "            menu_title=\"メインメニュー\",\n",
        "            options=_MENU_ITEMS,\n",
        "            icons=_ICONS,\n",
        "            menu_icon=\"cast\",\n",
        "            default_index=0,\n",
        "        )\n",
        "\n",
        "        # テーマ切替スイッチ\n",
        "        if \"light_mode\" not in st.session_state:\n",
        "            st.session_state.light_mode = True\n",
        "\n",
        "        if st.button(\"🌓 テーマ切替\"):\n",
        "            st.session_state.light_mode = not st.session_state.light_mode\n",
        "\n",
        "    # サイドバー外でテーマ適用（CSS は 1 度のみ挿入）\n",
        "    _apply_dark_theme(not st.session_state.light_mode)\n",
        "\n",
        "    return selected\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 共通ユーティリティ\n",
        "# ----------------------------------------------------------------------\n",
        "def page_hourglass() -> None:\n",
        "    \"\"\"デモ用のプログレスバーを表示するページ。\"\"\"\n",
        "    st.subheader(\"改善されたプログレス表示\")\n",
        "    progress_text = \"処理中です。しばらくお待ちください...\"\n",
        "    bar = st.progress(0, text=progress_text)\n",
        "    placeholder = st.empty()\n",
        "\n",
        "    for percent in range(100):\n",
        "        time.sleep(0.02)\n",
        "        bar.progress(percent + 1, text=f\"{progress_text} ({percent + 1}%)\")\n",
        "        if percent % 10 == 0:\n",
        "            placeholder.info(f\"Step {percent // 10 + 1}/10 完了\")\n",
        "\n",
        "    bar.empty()\n",
        "    placeholder.empty()\n",
        "    st.success(\"処理が完了しました！\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ページ切替ハブ\n",
        "# ----------------------------------------------------------------------\n",
        "def show_custom_pages(selected: str) -> str:\n",
        "    \"\"\"\n",
        "    選択されたページに対応するコンテンツを描画する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    selected : str\n",
        "        create_sidebar から返されたページ名。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        同じ値を返すだけ（呼び出し元での再利用用）。\n",
        "    \"\"\"\n",
        "    if selected == \"ホーム\":\n",
        "        _render_home()\n",
        "\n",
        "    # そのほかのページは app.py で処理\n",
        "    return selected\n",
        "\n",
        "\n",
        "def _render_home() -> None:\n",
        "    \"\"\"ホームページを描画。\"\"\"\n",
        "    st.title(\"Streamlit UIデモ（改善版）\")\n",
        "    st.write(\n",
        "        \"\"\"\n",
        "        このデモアプリは、Streamlit の基本的な UI 要素を紹介するものです。\n",
        "        サイドバーから異なるセクションを選択して、さまざまなコンポーネントをお試しください。\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # カード風に 3 カラムで機能案内\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.info(\"**基本要素**\\n\\nテキスト、ヘッダー、メディアなど\")\n",
        "    with col2:\n",
        "        st.success(\"**レイアウト**\\n\\n列、タブ、エキスパンダーなど\")\n",
        "    with col3:\n",
        "        st.warning(\"**入力要素**\\n\\nボタン、スライダー、テキスト入力など\")\n",
        "\n",
        "    # プログレスバーのデモ起動\n",
        "    if st.button(\"プログレスバーデモを表示\"):\n",
        "        page_hourglass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6aCDSi04WwI",
        "outputId": "97e9e1f5-e609-4134-f0a6-47e6ddc2e3c6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting custom_ui.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyw6VHaTELSN"
      },
      "source": [
        "ngrokのトークンを使用して、認証を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "aYw1q0iXELSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393d9179-e244-4452-bfde-50956165de5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**app.pyの書き換え**"
      ],
      "metadata": {
        "id": "3x2rBm74meW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\"\"\"Streamlit UI Demo — cleaned & refactored\n",
        "------------------------------------------------\n",
        "A single‑file demo app showcasing basic Streamlit components.\n",
        "Split into small render_* functions for readability.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import io\n",
        "from datetime import date as dt_date\n",
        "from typing import Callable, Dict\n",
        "\n",
        "import altair as alt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "from custom_ui import create_sidebar, show_custom_pages\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# ページ共通ユーティリティ\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def random_df(rows: int = 20, cols: int = 3, colnames: list[str] | None = None) -> pd.DataFrame:\n",
        "    \"\"\"ランダム DataFrame を生成。\"\"\"\n",
        "    if colnames is None:\n",
        "        colnames = list(\"ABC\")[:cols]\n",
        "    return pd.DataFrame(np.random.randn(rows, cols), columns=colnames)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 個別ページ描画関数\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def render_basic() -> None:\n",
        "    \"\"\"基本 UI 要素ページ\"\"\"\n",
        "    st.title(\"基本要素\")\n",
        "\n",
        "    # --- テキスト類 ---\n",
        "    st.header(\"テキスト要素\")\n",
        "    st.text(\"これは通常のテキストです。\")\n",
        "    st.markdown(\"**これはマークダウンテキストです。** *イタリック* や `コード` も使えます。\")\n",
        "    st.info(\"これは情報メッセージです。\")\n",
        "    st.warning(\"これは警告メッセージです。\")\n",
        "    st.error(\"これはエラーメッセージです。\")\n",
        "    st.success(\"これは成功メッセージです。\")\n",
        "\n",
        "    # --- メディア要素 ---\n",
        "    st.header(\"メディア要素\")\n",
        "    tab_chart, tab_df, tab_img = st.tabs([\"📈 チャート\", \"🗃 DataFrame\", \"🖼 画像\"])\n",
        "\n",
        "    with tab_chart:\n",
        "        st.subheader(\"折れ線グラフ\")\n",
        "        chart_data = random_df()\n",
        "        st.line_chart(chart_data)\n",
        "\n",
        "        st.subheader(\"Altair チャート\")\n",
        "        c = (\n",
        "            alt.Chart(chart_data.reset_index())\n",
        "            .mark_circle()\n",
        "            .encode(x=\"index\", y=\"A\", size=\"B\", color=\"C\", tooltip=[\"A\", \"B\", \"C\"])\n",
        "            .interactive()\n",
        "        )\n",
        "        st.altair_chart(c, use_container_width=True)\n",
        "\n",
        "    with tab_df:\n",
        "        st.subheader(\"DataFrame の表示\")\n",
        "        df = pd.DataFrame({\n",
        "            \"名前\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "            \"年齢\": [24, 42, 18, 31],\n",
        "            \"都市\": [\"東京\", \"大阪\", \"京都\", \"名古屋\"],\n",
        "        })\n",
        "        st.dataframe(df, use_container_width=True)\n",
        "        st.download_button(\"CSVとしてダウンロード\", df.to_csv(index=False, encoding=\"utf-8-sig\"), \"sample_data.csv\", \"text/csv\")\n",
        "\n",
        "    with tab_img:\n",
        "        st.subheader(\"動的に生成した画像\")\n",
        "        fig, ax = plt.subplots()\n",
        "        x = np.linspace(0, 10, 100)\n",
        "        ax.plot(x, np.sin(x))\n",
        "        ax.set_title(\"サイン波\")\n",
        "        ax.grid(True)\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format=\"png\")\n",
        "        st.image(buf.getvalue(), caption=\"動的に生成されたサイン波\", use_column_width=True)\n",
        "\n",
        "\n",
        "def render_layout() -> None:\n",
        "    \"\"\"レイアウト要素ページ\"\"\"\n",
        "    st.title(\"レイアウト要素\")\n",
        "\n",
        "    st.header(\"カラムレイアウト\")\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.subheader(\"カラム1\")\n",
        "        st.image(\"https://via.placeholder.com/150\", caption=\"プレースホルダー画像\")\n",
        "    with col2:\n",
        "        st.subheader(\"カラム2\")\n",
        "        st.metric(\"温度\", \"28°C\", \"1.2°C\")\n",
        "    with col3:\n",
        "        st.subheader(\"カラム3\")\n",
        "        st.metric(\"湿度\", \"65%\", \"-4%\", delta_color=\"inverse\")\n",
        "\n",
        "    st.header(\"エキスパンダー\")\n",
        "    with st.expander(\"詳細を表示\"):\n",
        "        st.write(\"\"\"\n",
        "            エキスパンダーを使用すると、長いコンテンツを折りたたむことができます。\n",
        "            ユーザーが必要なときに展開できるため、画面スペースを節約できます。\n",
        "        \"\"\")\n",
        "        st.image(\"https://via.placeholder.com/400x200\", caption=\"大きなプレースホルダー画像\")\n",
        "\n",
        "    st.header(\"タブ\")\n",
        "    tab1, tab2, tab3 = st.tabs([\"Tab 1\", \"Tab 2\", \"Tab 3\"])\n",
        "    with tab1:\n",
        "        st.bar_chart(random_df(10, 3, [\"X\", \"Y\", \"Z\"]))\n",
        "    with tab2:\n",
        "        st.line_chart(pd.DataFrame(np.sin(np.linspace(0, 10, 100))))\n",
        "    with tab3:\n",
        "        st.area_chart(random_df(10).cumsum())\n",
        "\n",
        "\n",
        "def render_inputs() -> None:\n",
        "    \"\"\"入力要素ページ\"\"\"\n",
        "    st.title(\"入力要素\")\n",
        "\n",
        "    # --- ボタン ---\n",
        "    st.header(\"ボタン\")\n",
        "    if st.button(\"クリックしてください\"):\n",
        "        st.success(\"ボタンがクリックされました！\")\n",
        "\n",
        "    # --- チェックボックス ---\n",
        "    st.header(\"チェックボックス\")\n",
        "    if st.checkbox(\"チェックボックスを表示\"):\n",
        "        st.write(\"チェックボックスがオンになっています。\")\n",
        "\n",
        "    # --- ラジオボタン ---\n",
        "    st.header(\"ラジオボタン\")\n",
        "    genre = st.radio(\"好きな音楽ジャンルは？\", (\"ロック\", \"ポップ\", \"ジャズ\", \"クラシック\"))\n",
        "    if genre:\n",
        "        st.write(f\"あなたは {genre} を選択しました。\")\n",
        "\n",
        "    # --- セレクトボックス ---\n",
        "    st.header(\"セレクトボックス\")\n",
        "    color = st.selectbox(\"好きな色は？\", (\"赤\", \"青\", \"緑\", \"黄色\"))\n",
        "    st.write(f\"あなたが選んだのは: {color}\")\n",
        "\n",
        "    # --- マルチセレクト ---\n",
        "    st.header(\"マルチセレクト\")\n",
        "    fruits = st.multiselect(\n",
        "        \"好きな果物は？\",\n",
        "        [\"りんご\", \"バナナ\", \"オレンジ\", \"ぶどう\", \"いちご\"],\n",
        "        default=[\"りんご\", \"バナナ\"],\n",
        "    )\n",
        "    st.write(\"あなたが選んだのは: \" + \", \".join(fruits))\n",
        "\n",
        "    # --- スライダー ---\n",
        "    st.header(\"スライダー\")\n",
        "    age = st.slider(\"あなたの年齢は？\", 0, 100, 25)\n",
        "    st.write(f\"あなたの年齢: {age}歳\")\n",
        "\n",
        "    # --- 範囲スライダー ---\n",
        "    values = st.slider(\"値の範囲を選択:\", 0.0, 100.0, (25.0, 75.0))\n",
        "    st.write(f\"選択された範囲: {values[0]} から {values[1]}\")\n",
        "\n",
        "    # --- 日付入力 ---\n",
        "    st.header(\"日付入力\")\n",
        "    birth = st.date_input(\"生年月日を選択してください\", dt_date(2000, 1, 1))\n",
        "    st.write(f\"あなたの生年月日: {birth}\")\n",
        "\n",
        "    # --- ファイルアップローダー ---\n",
        "    st.header(\"ファイルアップローダー\")\n",
        "    uploaded = st.file_uploader(\"ファイルを選択してください\", type=[\"csv\", \"xlsx\", \"txt\", \"jpg\", \"png\"])\n",
        "    if uploaded is not None:\n",
        "        st.write(f\"ファイル名: {uploaded.name} — {uploaded.size} bytes\")\n",
        "        if uploaded.type.startswith(\"image\"):\n",
        "            st.image(uploaded, caption=\"アップロードされた画像\", use_column_width=True)\n",
        "        elif uploaded.type == \"text/plain\":\n",
        "            string_data = io.StringIO(uploaded.getvalue().decode()).read()\n",
        "            st.text_area(\"ファイルの内容\", string_data, height=200)\n",
        "        elif uploaded.type == \"text/csv\":\n",
        "            st.dataframe(pd.read_csv(uploaded), use_container_width=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# テーマ設定ページは custom_ui.py 内の CSS で制御\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# ページ設定 & ルーティング\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def main() -> None:\n",
        "    st.set_page_config(\n",
        "        page_title=\"Streamlit UIデモ（改善版）\",\n",
        "        page_icon=\"🧊\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\",\n",
        "    )\n",
        "\n",
        "    selected = create_sidebar()\n",
        "    selected = show_custom_pages(selected)  # ホームなど custom_ui 側\n",
        "\n",
        "    page_table: Dict[str, Callable[[], None]] = {\n",
        "        \"基本要素\": render_basic,\n",
        "        \"レイアウト\": render_layout,\n",
        "        \"入力要素\": render_inputs,\n",
        "    }\n",
        "\n",
        "    if selected in page_table:\n",
        "        page_table[selected]()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqPQcRxLmikL",
        "outputId": "b02a3da3-ceeb-465d-c8cd-56b833379189"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RssTcD_IELSN"
      },
      "source": [
        "アプリを起動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "f-E7ucR6ELSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9621cd0b-919d-4d0d-92f0-6d4d50332606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "公開URL: https://f89e-34-125-210-255.ngrok-free.app\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.210.255:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 12469 (\\N{KATAKANA LETTER SA}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 12452 (\\N{KATAKANA LETTER I}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 12531 (\\N{KATAKANA LETTER N}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "/content/lecture-ai-engineering/day1/01_streamlit_UI/app.py:84: UserWarning: Glyph 27874 (\\N{CJK UNIFIED IDEOGRAPH-6CE2}) missing from font(s) DejaVu Sans.\n",
            "  fig.savefig(buf, format=\"png\")\n",
            "2025-04-21 05:45:37.838 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"公開URL: {public_url}\")\n",
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbYyXVFjELSN"
      },
      "source": [
        "公開URLの後に記載されているURLにブラウザでアクセスすると、streamlitのUIが表示されます。\n",
        "\n",
        "app.pyのコメントアウトされている箇所を編集することで、UIがどの様に変化するか確認してみましょう。\n",
        "\n",
        "streamlitの公式ページには、ギャラリーページがあります。\n",
        "\n",
        "streamlitを使うとpythonという一つの言語であっても、様々なUIを実現できることがわかると思います。\n",
        "\n",
        "https://streamlit.io/gallery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmtP5GLOELSN"
      },
      "source": [
        "後片付けとして、使う必要のないngrokのトンネルを削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8Ek9QgahELSO"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-T8tFpyELSO"
      },
      "source": [
        "# 02_streamlit_app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqogFQKnELSO"
      },
      "source": [
        "\n",
        "ディレクトリ「02_streamlit_app」に移動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "UeEjlJ7uELSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa30922-ceef-4b57-eb8a-2379a0cf0e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/02_streamlit_app\n"
          ]
        }
      ],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/02_streamlit_app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XUH2AstELSO"
      },
      "source": [
        "必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "mDqvI4V3ELSO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO31umGZELSO"
      },
      "source": [
        "ngrokとhuggigfaceのトークンを使用して、認証を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jPxTiEWQELSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5099379-14d1-4b42-d6cd-208898c564d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `AIE_2` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `AIE_2`\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz4WrELLELSP"
      },
      "source": [
        "stramlitでHuggingfaceのトークン情報を扱うために、streamlit用の設定ファイル（.streamlit）を作成し、トークンの情報を格納します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "W184-a7qFP0W"
      },
      "outputs": [],
      "source": [
        "# .streamlit/secrets.toml ファイルを作成\n",
        "import os\n",
        "import toml\n",
        "\n",
        "# 設定ファイルのディレクトリ確保\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "\n",
        "# 環境変数から取得したトークンを設定ファイルに書き込む\n",
        "secrets = {\n",
        "    \"huggingface\": {\n",
        "        \"token\": os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n",
        "    }\n",
        "}\n",
        "\n",
        "# 設定ファイルを書き込む\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    toml.dump(secrets, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK0vI_xKELSP"
      },
      "source": [
        "アプリを起動します。\n",
        "\n",
        "02_streamlit_appでは、Huggingfaceからモデルをダウンロードするため、初回起動には2分程度時間がかかります。\n",
        "\n",
        "この待ち時間を利用して、app.pyのコードを確認してみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**metrics.py の書き換え**"
      ],
      "metadata": {
        "id": "HhaRy8Bt6f4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile llm.py\n",
        "# import time\n",
        "# import numpy as np\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from textblob import TextBlob\n",
        "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "# import nltk\n",
        "# import math\n",
        "# import re\n",
        "\n",
        "# # 必要なNLTKデータのダウンロード\n",
        "# try:\n",
        "#     nltk.download('punkt', quiet=True)\n",
        "# except:\n",
        "#     pass\n",
        "\n",
        "# class MetricsCalculator:\n",
        "#     def __init__(self):\n",
        "#         self.vectorizer = TfidfVectorizer()\n",
        "#         self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#     def calculate_bleu_score(self, reference, candidate):\n",
        "#         \"\"\"BLEUスコアを計算 (0-1, 高いほど良い)\"\"\"\n",
        "#         reference_tokens = nltk.word_tokenize(reference.lower())\n",
        "#         candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
        "#         return sentence_bleu([reference_tokens], candidate_tokens)\n",
        "\n",
        "#     def calculate_cosine_similarity(self, text1, text2):\n",
        "#         \"\"\"コサイン類似度を計算 (0-1, 高いほど似ている)\"\"\"\n",
        "#         try:\n",
        "#             tfidf_matrix = self.vectorizer.fit_transform([text1, text2])\n",
        "#             return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "#         except:\n",
        "#             return 0.0\n",
        "\n",
        "#     def calculate_sentiment_score(self, text):\n",
        "#         \"\"\"感情分析スコアを計算 (-1: 非常にネガティブ, 1: 非常にポジティブ)\"\"\"\n",
        "#         return self.sentiment_analyzer.polarity_scores(text)[\"compound\"]\n",
        "\n",
        "#     def calculate_sentiment_metrics(self, text):\n",
        "#         \"\"\"詳細な感情分析メトリクスを計算\"\"\"\n",
        "#         sentiment = TextBlob(text).sentiment\n",
        "#         vader_sentiment = self.sentiment_analyzer.polarity_scores(text)\n",
        "\n",
        "#         return {\n",
        "#             \"polarity\": sentiment.polarity,  # -1.0 to 1.0\n",
        "#             \"subjectivity\": sentiment.subjectivity,  # 0.0 to 1.0\n",
        "#             \"vader_compound\": vader_sentiment[\"compound\"],\n",
        "#             \"vader_negative\": vader_sentiment[\"neg\"],\n",
        "#             \"vader_neutral\": vader_sentiment[\"neu\"],\n",
        "#             \"vader_positive\": vader_sentiment[\"pos\"]\n",
        "#         }\n",
        "\n",
        "#     def calculate_perplexity(self, text, n=3):\n",
        "#         \"\"\"単純なN-gramモデルに基づくテキストの複雑さ推定（低いほど自然）\"\"\"\n",
        "#         tokens = nltk.word_tokenize(text.lower())\n",
        "#         if len(tokens) < n:\n",
        "#             return float('inf')  # テキストが短すぎる場合\n",
        "\n",
        "#         # N-gramカウント\n",
        "#         ngrams = {}\n",
        "#         for i in range(len(tokens) - n + 1):\n",
        "#             gram = ' '.join(tokens[i:i+n-1])\n",
        "#             next_token = tokens[i+n-1]\n",
        "\n",
        "#             if gram not in ngrams:\n",
        "#                 ngrams[gram] = {}\n",
        "\n",
        "#             if next_token not in ngrams[gram]:\n",
        "#                 ngrams[gram][next_token] = 0\n",
        "\n",
        "#             ngrams[gram][next_token] += 1\n",
        "\n",
        "#         # パープレキシティ計算\n",
        "#         log_prob = 0.0\n",
        "#         for i in range(len(tokens) - n + 1):\n",
        "#             gram = ' '.join(tokens[i:i+n-1])\n",
        "#             next_token = tokens[i+n-1]\n",
        "\n",
        "#             if gram in ngrams and next_token in ngrams[gram]:\n",
        "#                 total = sum(ngrams[gram].values())\n",
        "#                 prob = ngrams[gram][next_token] / total\n",
        "#                 log_prob += math.log2(prob)\n",
        "#             else:\n",
        "#                 log_prob += math.log2(1e-10)  # スムージング\n",
        "\n",
        "#         perplexity = 2 ** (-log_prob / (len(tokens) - n + 1))\n",
        "#         return perplexity\n",
        "\n",
        "#     def calculate_response_time(self, start_time):\n",
        "#         \"\"\"応答時間を計算（秒単位）\"\"\"\n",
        "#         return time.time() - start_time\n",
        "\n",
        "#     def calculate_token_generation_speed(self, text, generation_time):\n",
        "#         \"\"\"トークン生成速度を計算（トークン/秒）\"\"\"\n",
        "#         if generation_time <= 0:\n",
        "#             return 0\n",
        "#         # 簡易的なトークン数推定（実際にはモデルのトークナイザーによって異なる）\n",
        "#         token_count = len(re.findall(r'\\w+|[^\\w\\s]', text))\n",
        "#         return token_count / generation_time\n",
        "\n",
        "#     def calculate_all_metrics(self, reference, candidate, generation_time):\n",
        "#         \"\"\"すべての評価指標を計算\"\"\"\n",
        "#         metrics = {\n",
        "#             \"bleu_score\": self.calculate_bleu_score(reference, candidate),\n",
        "#             \"cosine_similarity\": self.calculate_cosine_similarity(reference, candidate),\n",
        "#             \"sentiment\": self.calculate_sentiment_metrics(candidate),\n",
        "#             \"response_time\": generation_time,\n",
        "#             \"token_generation_speed\": self.calculate_token_generation_speed(candidate, generation_time),\n",
        "#             \"perplexity\": self.calculate_perplexity(candidate)\n",
        "#         }\n",
        "#         return metrics\n",
        "\n",
        "# # 使用例\n",
        "# if __name__ == \"__main__\":\n",
        "#     calculator = MetricsCalculator()\n",
        "#     reference = \"これは参照テキストです。\"\n",
        "#     candidate = \"これは生成されたテキストです。\"\n",
        "#     start_time = time.time()\n",
        "#     time.sleep(1)  # 生成時間のシミュレーション\n",
        "#     generation_time = time.time() - start_time\n",
        "\n",
        "#     metrics = calculator.calculate_all_metrics(reference, candidate, generation_time)\n",
        "#     print(metrics)"
      ],
      "metadata": {
        "id": "hBCnjqZ26ktb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**llm.pyの書き換え**"
      ],
      "metadata": {
        "id": "jJahIGtm6crr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile llm.py\n",
        "# import streamlit as st\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "# from transformers_stream_generator import init_streamer\n",
        "# import time\n",
        "# from config import MODEL_NAME, STREAM_OUTPUT, ENABLE_CACHING\n",
        "\n",
        "# class LLMGenerator:\n",
        "#     def __init__(self):\n",
        "#         self.model = None\n",
        "#         self.tokenizer = None\n",
        "#         self.load_model()\n",
        "\n",
        "#     @st.cache_resource\n",
        "#     def load_model_cached(_self):\n",
        "#         \"\"\"モデルをロードしてキャッシュする\"\"\"\n",
        "#         print(f\"モデル {MODEL_NAME} をロード中...\")\n",
        "\n",
        "#         # 量子化設定\n",
        "#         quantization_config = BitsAndBytesConfig(\n",
        "#             load_in_4bit=True,\n",
        "#             bnb_4bit_compute_dtype=torch.float16\n",
        "#         )\n",
        "\n",
        "#         # トークナイザーとモデルのロード\n",
        "#         tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "#         # モデルのロード（4bit量子化）\n",
        "#         model = AutoModelForCausalLM.from_pretrained(\n",
        "#             MODEL_NAME,\n",
        "#             quantization_config=quantization_config,\n",
        "#             device_map=\"auto\",\n",
        "#             torch_dtype=torch.float16\n",
        "#         )\n",
        "\n",
        "#         return model, tokenizer\n",
        "\n",
        "#     def load_model(self):\n",
        "#         \"\"\"モデルのロード処理（キャッシュが有効な場合はキャッシュを使用）\"\"\"\n",
        "#         if ENABLE_CACHING:\n",
        "#             self.model, self.tokenizer = self.load_model_cached()\n",
        "#         else:\n",
        "#             print(f\"モデル {MODEL_NAME} をロード中...\")\n",
        "\n",
        "#             # 量子化設定\n",
        "#             quantization_config = BitsAndBytesConfig(\n",
        "#                 load_in_4bit=True,\n",
        "#                 bnb_4bit_compute_dtype=torch.float16\n",
        "#             )\n",
        "\n",
        "#             # トークナイザーとモデルのロード\n",
        "#             self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "#             # モデルのロード（4bit量子化）\n",
        "#             self.model = AutoModelForCausalLM.from_pretrained(\n",
        "#                 MODEL_NAME,\n",
        "#                 quantization_config=quantization_config,\n",
        "#                 device_map=\"auto\",\n",
        "#                 torch_dtype=torch.float16\n",
        "#             )\n",
        "\n",
        "#     def generate_text(self, prompt, max_length=512, temperature=0.7, stream_handler=None):\n",
        "#         \"\"\"テキスト生成（ストリーミングあり/なし両方対応）\"\"\"\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         try:\n",
        "#             # プロンプトのフォーマット\n",
        "#             if \"meta-llama\" in MODEL_NAME.lower():\n",
        "#                 formatted_prompt = f\"<human>: {prompt}\\n<assistant>: \"\n",
        "#             else:\n",
        "#                 formatted_prompt = f\"ユーザー: {prompt}\\nシステム: \"\n",
        "\n",
        "#             # 入力のエンコード\n",
        "#             inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "#             generation_args = {\n",
        "#                 \"max_new_tokens\": max_length,\n",
        "#                 \"temperature\": temperature,\n",
        "#                 \"top_p\": 0.95,\n",
        "#                 \"top_k\": 50,\n",
        "#                 \"repetition_penalty\": 1.1,\n",
        "#                 \"do_sample\": temperature > 0.1,\n",
        "#                 \"pad_token_id\": self.tokenizer.eos_token_id\n",
        "#             }\n",
        "\n",
        "#             if STREAM_OUTPUT and stream_handler:\n",
        "#                 # ストリーミング生成\n",
        "#                 streamer = init_streamer(self.model, self.tokenizer)\n",
        "\n",
        "#                 # 生成開始（非同期）\n",
        "#                 generation_task = self.model.generate(\n",
        "#                     **inputs,\n",
        "#                     streamer=streamer,\n",
        "#                     **generation_args\n",
        "#                 )\n",
        "\n",
        "#                 # ストリーマーからトークンを取得してハンドラに渡す\n",
        "#                 generated_text = \"\"\n",
        "#                 for new_text in streamer:\n",
        "#                     generated_text += new_text\n",
        "#                     if stream_handler:\n",
        "#                         stream_handler(new_text)\n",
        "\n",
        "#                 # 生成完了を待機\n",
        "#                 try:\n",
        "#                     generation_task.result()\n",
        "#                 except:\n",
        "#                     pass\n",
        "\n",
        "#             else:\n",
        "#                 # 非ストリーミング生成\n",
        "#                 output = self.model.generate(\n",
        "#                     **inputs,\n",
        "#                     **generation_args\n",
        "#                 )\n",
        "\n",
        "#                 # 出力のデコード\n",
        "#                 generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "#                 # プロンプト部分を削除\n",
        "#                 generated_text = generated_text.replace(formatted_prompt, \"\")\n",
        "\n",
        "#             generation_time = time.time() - start_time\n",
        "#             return generated_text, generation_time\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"テキスト生成中にエラーが発生しました: {e}\")\n",
        "#             generation_time = time.time() - start_time\n",
        "#             return f\"エラーが発生しました: {e}\", generation_time\n",
        "\n",
        "# # 使用例\n",
        "# if __name__ == \"__main__\":\n",
        "#     generator = LLMGenerator()\n",
        "#     response, gen_time = generator.generate_text(\"こんにちは、元気ですか？\")\n",
        "#     print(f\"応答: {response}\")\n",
        "#     print(f\"生成時間: {gen_time:.2f}秒\")"
      ],
      "metadata": {
        "id": "APfi3UtA6HcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**custom_ui.py 追加**"
      ],
      "metadata": {
        "id": "f1IJsTLP6v2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile custom_ui.py\n",
        "# import streamlit as st\n",
        "# from streamlit_option_menu import option_menu\n",
        "\n",
        "# def create_sidebar():\n",
        "#     \"\"\"カスタムサイドバーを作成する関数\"\"\"\n",
        "#     with st.sidebar:\n",
        "#         selected = option_menu(\n",
        "#             \"メインメニュー\",\n",
        "#             [\"ホーム\", \"基本要素\", \"レイアウト\", \"入力要素\", \"テーマ設定\"],\n",
        "#             icons=[\"house\", \"list-task\", \"columns\", \"input-cursor\", \"palette\"],\n",
        "#             menu_icon=\"cast\",\n",
        "#             default_index=0,\n",
        "#         )\n",
        "\n",
        "#         # ダークモード/ライトモードの切り替え\n",
        "#         if \"light_mode\" not in st.session_state:\n",
        "#             st.session_state.light_mode = True\n",
        "\n",
        "#         if st.button(\"🌓 テーマ切替\"):\n",
        "#             st.session_state.light_mode = not st.session_state.light_mode\n",
        "\n",
        "#         # カスタムCSS\n",
        "#         if not st.session_state.light_mode:\n",
        "#             st.markdown(\"\"\"\n",
        "#             <style>\n",
        "#             .main {background-color: #0E1117; color: white;}\n",
        "#             .sidebar .sidebar-content {background-color: #262730; color: white;}\n",
        "#             </style>\n",
        "#             \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "#         return selected\n",
        "\n",
        "# def page_hourglass():\n",
        "#     \"\"\"プログレスバーデモページ\"\"\"\n",
        "#     import time\n",
        "\n",
        "#     st.subheader(\"改善されたプログレス表示\")\n",
        "#     progress_text = \"処理中です。しばらくお待ちください...\"\n",
        "#     my_bar = st.progress(0, text=progress_text)\n",
        "#     placeholder = st.empty()\n",
        "\n",
        "#     for percent_complete in range(100):\n",
        "#         time.sleep(0.02)\n",
        "#         my_bar.progress(percent_complete + 1, text=f\"{progress_text} ({percent_complete+1}%)\")\n",
        "#         if percent_complete % 10 == 0:\n",
        "#             placeholder.info(f\"Step {percent_complete // 10 + 1}/10 完了\")\n",
        "\n",
        "#     my_bar.empty()\n",
        "#     placeholder.empty()\n",
        "#     st.success(\"処理が完了しました！\")\n",
        "\n",
        "# def show_custom_pages(selected):\n",
        "#     \"\"\"選択されたページに基づいてコンテンツを表示\"\"\"\n",
        "#     if selected == \"ホーム\":\n",
        "#         st.title(\"Streamlit UIデモ（改善版）\")\n",
        "#         st.write(\"\"\"このデモアプリは、Streamlitの基本的なUI要素を紹介するものです。\n",
        "#         サイドバーから異なるセクションを選択して、様々なStreamlitコンポーネントを試してみてください。\"\"\")\n",
        "\n",
        "#         # カード要素の追加\n",
        "#         col1, col2, col3 = st.columns(3)\n",
        "#         with col1:\n",
        "#             st.info(\"**基本要素**\\n\\nテキスト、ヘッダー、メディアなどの基本的なUI要素\")\n",
        "#         with col2:\n",
        "#             st.success(\"**レイアウト**\\n\\n列、タブ、エキスパンダーなどのレイアウトオプション\")\n",
        "#         with col3:\n",
        "#             st.warning(\"**入力要素**\\n\\nボタン、スライダー、テキスト入力などのインタラクティブ要素\")\n",
        "\n",
        "#         # プログレスバーデモの表示\n",
        "#         if st.button(\"プログレスバーデモを表示\"):\n",
        "#             page_hourglass()\n",
        "\n",
        "#     # 他のページの実装はapp.pyに任せる\n",
        "#     return selected"
      ],
      "metadata": {
        "id": "O-soh1H-60kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**database.py 書き換え**"
      ],
      "metadata": {
        "id": "zoRyejCN7IYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile database.py\n",
        "# import sqlite3\n",
        "# import json\n",
        "# import time\n",
        "# from datetime import datetime\n",
        "\n",
        "# class ChatDatabase:\n",
        "#     def __init__(self, db_file):\n",
        "#         self.db_file = db_file\n",
        "#         self.initialize_db()\n",
        "\n",
        "#     def initialize_db(self):\n",
        "#         \"\"\"データベースとテーブルの初期化\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         # チャット履歴テーブル\n",
        "#         cursor.execute('''\n",
        "#         CREATE TABLE IF NOT EXISTS chat_history (\n",
        "#             id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#             session_id TEXT,\n",
        "#             timestamp TEXT,\n",
        "#             user_input TEXT,\n",
        "#             model_response TEXT,\n",
        "#             response_time REAL,\n",
        "#             metrics TEXT\n",
        "#         )\n",
        "#         ''')\n",
        "\n",
        "#         # フィードバックテーブル\n",
        "#         cursor.execute('''\n",
        "#         CREATE TABLE IF NOT EXISTS feedback (\n",
        "#             id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#             chat_id INTEGER,\n",
        "#             rating INTEGER,\n",
        "#             feedback_text TEXT,\n",
        "#             timestamp TEXT,\n",
        "#             FOREIGN KEY (chat_id) REFERENCES chat_history (id)\n",
        "#         )\n",
        "#         ''')\n",
        "\n",
        "#         # エラーログテーブル（新規追加）\n",
        "#         cursor.execute('''\n",
        "#         CREATE TABLE IF NOT EXISTS error_logs (\n",
        "#             id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#             timestamp TEXT,\n",
        "#             error_type TEXT,\n",
        "#             error_message TEXT,\n",
        "#             stack_trace TEXT,\n",
        "#             input_data TEXT\n",
        "#         )\n",
        "#         ''')\n",
        "\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#     def save_chat(self, session_id, user_input, model_response, response_time, metrics=None):\n",
        "#         \"\"\"チャット履歴を保存\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         timestamp = datetime.now().isoformat()\n",
        "#         metrics_json = json.dumps(metrics) if metrics else \"{}\"\n",
        "\n",
        "#         cursor.execute(\n",
        "#             \"INSERT INTO chat_history (session_id, timestamp, user_input, model_response, response_time, metrics) VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "#             (session_id, timestamp, user_input, model_response, response_time, metrics_json)\n",
        "#         )\n",
        "\n",
        "#         chat_id = cursor.lastrowid\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#         return chat_id\n",
        "\n",
        "#     def save_feedback(self, chat_id, rating, feedback_text=\"\"):\n",
        "#         \"\"\"フィードバックを保存\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         timestamp = datetime.now().isoformat()\n",
        "\n",
        "#         cursor.execute(\n",
        "#             \"INSERT INTO feedback (chat_id, rating, feedback_text, timestamp) VALUES (?, ?, ?, ?)\",\n",
        "#             (chat_id, rating, feedback_text, timestamp)\n",
        "#         )\n",
        "\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#     def get_chat_history(self, session_id=None, limit=10, offset=0):\n",
        "#         \"\"\"チャット履歴を取得\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         conn.row_factory = sqlite3.Row\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         if session_id:\n",
        "#             cursor.execute(\n",
        "#                 \"SELECT * FROM chat_history WHERE session_id = ? ORDER BY timestamp DESC LIMIT ? OFFSET ?\",\n",
        "#                 (session_id, limit, offset)\n",
        "#             )\n",
        "#         else:\n",
        "#             cursor.execute(\n",
        "#                 \"SELECT * FROM chat_history ORDER BY timestamp DESC LIMIT ? OFFSET ?\",\n",
        "#                 (limit, offset)\n",
        "#             )\n",
        "\n",
        "#         rows = cursor.fetchall()\n",
        "#         history = []\n",
        "\n",
        "#         for row in rows:\n",
        "#             feedback_cursor = conn.cursor()\n",
        "#             feedback_cursor.execute(\n",
        "#                 \"SELECT rating, feedback_text FROM feedback WHERE chat_id = ?\",\n",
        "#                 (row['id'],)\n",
        "#             )\n",
        "#             feedback = feedback_cursor.fetchone()\n",
        "\n",
        "#             chat_item = dict(row)\n",
        "#             if feedback:\n",
        "#                 chat_item['feedback_rating'] = feedback['rating']\n",
        "#                 chat_item['feedback_text'] = feedback['feedback_text']\n",
        "#             else:\n",
        "#                 chat_item['feedback_rating'] = None\n",
        "#                 chat_item['feedback_text'] = None\n",
        "\n",
        "#             try:\n",
        "#                 chat_item['metrics'] = json.loads(chat_item['metrics'])\n",
        "#             except:\n",
        "#                 chat_item['metrics'] = {}\n",
        "\n",
        "#             history.append(chat_item)\n",
        "\n",
        "#         conn.close()\n",
        "#         return history\n",
        "\n",
        "#     def get_total_chat_count(self, session_id=None):\n",
        "#         \"\"\"チャット履歴の総数を取得\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         if session_id:\n",
        "#             cursor.execute(\n",
        "#                 \"SELECT COUNT(*) FROM chat_history WHERE session_id = ?\",\n",
        "#                 (session_id,)\n",
        "#             )\n",
        "#         else:\n",
        "#             cursor.execute(\"SELECT COUNT(*) FROM chat_history\")\n",
        "\n",
        "#         count = cursor.fetchone()[0]\n",
        "#         conn.close()\n",
        "#         return count\n",
        "\n",
        "#     def log_error(self, error_type, error_message, stack_trace=\"\", input_data=\"\"):\n",
        "#         \"\"\"エラーをログに記録（新規追加）\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         timestamp = datetime.now().isoformat()\n",
        "\n",
        "#         cursor.execute(\n",
        "#             \"INSERT INTO error_logs (timestamp, error_type, error_message, stack_trace, input_data) VALUES (?, ?, ?, ?, ?)\",\n",
        "#             (timestamp, error_type, error_message, stack_trace, input_data)\n",
        "#         )\n",
        "\n",
        "#         conn.commit()\n",
        "#         conn.close()\n",
        "\n",
        "#     def get_statistics(self):\n",
        "#         \"\"\"チャット統計情報を取得（新規追加）\"\"\"\n",
        "#         conn = sqlite3.connect(self.db_file)\n",
        "#         conn.row_factory = sqlite3.Row\n",
        "#         cursor = conn.cursor()\n",
        "\n",
        "#         # 平均応答時間\n",
        "#         cursor.execute(\"SELECT AVG(response_time) as avg_response_time FROM chat_history\")\n",
        "#         avg_response_time = cursor.fetchone()['avg_response_time'] or 0\n",
        "\n",
        "#         # 総チャット数\n",
        "#         cursor.execute(\"SELECT COUNT(*) as total_chats FROM chat_history\")\n",
        "#         total_chats = cursor.fetchone()['total_chats'] or 0\n",
        "\n",
        "#         # 平均評価\n",
        "#         cursor.execute(\"SELECT AVG(rating) as avg_rating FROM feedback\")\n",
        "#         avg_rating = cursor.fetchone()['avg_rating'] or 0\n",
        "\n",
        "#         # 評価分布\n",
        "#         cursor.execute(\"\"\"\n",
        "#             SELECT rating, COUNT(*) as count\n",
        "#             FROM feedback\n",
        "#             GROUP BY rating\n",
        "#             ORDER BY rating\n",
        "#         \"\"\")\n",
        "#         rating_distribution = {row['rating']: row['count'] for row in cursor.fetchall()}\n",
        "\n",
        "#         conn.close()\n",
        "\n",
        "#         return {\n",
        "#             'avg_response_time': avg_response_time,\n",
        "#             'total_chats': total_chats,\n",
        "#             'avg_rating': avg_rating,\n",
        "#             'rating_distribution': rating_distribution\n",
        "#         }\n",
        "\n",
        "# # 使用例\n",
        "# if __name__ == \"__main__\":\n",
        "#     db = ChatDatabase(\"test.db\")\n",
        "#     chat_id = db.save_chat(\"test_session\", \"こんにちは\", \"こんにちは！\", 0.5, {\"bleu_score\": 0.8})\n",
        "#     db.save_feedback(chat_id, 5, \"とても良い応答でした\")\n",
        "#     history = db.get_chat_history(\"test_session\")\n",
        "#     print(history)"
      ],
      "metadata": {
        "id": "o-rIp1VF7L1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**app.py 書き換え**"
      ],
      "metadata": {
        "id": "Q1hK7yN07WwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import uuid\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import time\n",
        "# import io\n",
        "# import json\n",
        "# from datetime import datetime\n",
        "\n",
        "# from llm import LLMGenerator\n",
        "# from database import ChatDatabase\n",
        "# from metrics import MetricsCalculator\n",
        "# from config import DATABASE_FILE, THEME_COLOR, ENABLE_DARK_MODE, STREAM_OUTPUT, ENABLE_CHAT_EXPORT\n",
        "\n",
        "# # セッションステートの初期化\n",
        "# if 'session_id' not in st.session_state:\n",
        "#    st.session_state.session_id = str(uuid.uuid4())\n",
        "# if 'chat_history' not in st.session_state:\n",
        "#    st.session_state.chat_history = []\n",
        "# if 'llm_generator' not in st.session_state:\n",
        "#    st.session_state.llm_generator = LLMGenerator()\n",
        "# if 'database' not in st.session_state:\n",
        "#    st.session_state.database = ChatDatabase(DATABASE_FILE)\n",
        "# if 'metrics_calculator' not in st.session_state:\n",
        "#    st.session_state.metrics_calculator = MetricsCalculator()\n",
        "\n",
        "# # ページ設定\n",
        "# st.set_page_config(\n",
        "#    page_title=\"LLM Chat App\",\n",
        "#    page_icon=\"🤖\",\n",
        "#    layout=\"wide\",\n",
        "#    initial_sidebar_state=\"expanded\",\n",
        "# )\n",
        "\n",
        "# # ダークモードの適用\n",
        "# if ENABLE_DARK_MODE:\n",
        "#    st.markdown(\"\"\"\n",
        "#    <style>\n",
        "#    .main {background-color: #0E1117; color: white;}\n",
        "#    .sidebar .sidebar-content {background-color: #262730; color: white;}\n",
        "#    .stButton button {background-color: #4CAF50; color: white;}\n",
        "#    .stTextInput, .stTextArea {background-color: #262730; color: white;}\n",
        "#    </style>\n",
        "#    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# # サイドバーの作成\n",
        "# with st.sidebar:\n",
        "#    st.title(\"LLM Chat App\")\n",
        "\n",
        "#    # メニュー選択\n",
        "#    page = st.radio(\n",
        "#        \"メニュー\",\n",
        "#        [\"💬 チャット\", \"📚 履歴\", \"📊 分析\"]\n",
        "#    )\n",
        "\n",
        "#    # モデル設定\n",
        "#    if page == \"💬 チャット\":\n",
        "#        st.subheader(\"モデル設定\")\n",
        "\n",
        "#        # 温度の調整\n",
        "#        temperature = st.slider(\n",
        "#            \"温度 (創造性)\",\n",
        "#            min_value=0.1,\n",
        "#            max_value=1.0,\n",
        "#            value=0.7,\n",
        "#            step=0.1,\n",
        "#            help=\"値が高いほど創造的で多様な応答になります。低いと決定論的な応答になります。\"\n",
        "#        )\n",
        "\n",
        "#        # 最大長の調整\n",
        "#        max_length = st.slider(\n",
        "#            \"最大応答長\",\n",
        "#            min_value=64,\n",
        "#            max_value=1024,\n",
        "#            value=512,\n",
        "#            step=64,\n",
        "#            help=\"生成されるテキストの最大トークン数\"\n",
        "#        )\n",
        "\n",
        "#        # セッションリセット\n",
        "#        if st.button(\"新しい会話を開始\"):\n",
        "#            st.session_state.chat_history = []\n",
        "#            st.session_state.session_id = str(uuid.uuid4())\n",
        "#            st.success(\"新しい会話を開始しました\")\n",
        "\n",
        "# # チャットページ\n",
        "# if page == \"💬 チャット\":\n",
        "#    st.title(\"LLMチャット\")\n",
        "\n",
        "#    # チャット履歴の表示\n",
        "#    for chat in st.session_state.chat_history:\n",
        "#        # ユーザーメッセージ\n",
        "#        with st.chat_message(\"user\"):\n",
        "#            st.write(chat[\"user_input\"])\n",
        "\n",
        "#        # アシスタントメッセージ\n",
        "#        with st.chat_message(\"assistant\"):\n",
        "#            st.write(chat[\"model_response\"])\n",
        "\n",
        "#            # メトリクスの表示（折りたたみ可能）\n",
        "#            if \"metrics\" in chat and chat[\"metrics\"]:\n",
        "#                with st.expander(\"パフォーマンス指標\"):\n",
        "#                    metrics = chat[\"metrics\"]\n",
        "\n",
        "#                    # 基本メトリクス\n",
        "#                    cols = st.columns(3)\n",
        "#                    with cols[0]:\n",
        "#                        st.metric(\"応答時間\", f\"{metrics['response_time']:.2f}秒\")\n",
        "#                    with cols[1]:\n",
        "#                        st.metric(\"トークン生成速度\", f\"{metrics.get('token_generation_speed', 0):.1f} t/s\")\n",
        "#                    with cols[2]:\n",
        "#                        sentiment = metrics.get('sentiment', {}).get('vader_compound', 0)\n",
        "#                        st.metric(\"感情スコア\", f\"{sentiment:.2f}\", delta=sentiment)\n",
        "\n",
        "#                    # 詳細メトリクス\n",
        "#                    if 'sentiment' in metrics:\n",
        "#                        sentiment = metrics['sentiment']\n",
        "#                        st.write(\"**感情分析:**\")\n",
        "#                        sentiment_df = pd.DataFrame({\n",
        "#                            '指標': ['ポジティブ', 'ネガティブ', '中立', '主観性'],\n",
        "#                            '値': [\n",
        "#                                sentiment.get('vader_positive', 0),\n",
        "#                                sentiment.get('vader_negative', 0),\n",
        "#                                sentiment.get('vader_neutral', 0),\n",
        "#                                sentiment.get('subjectivity', 0)\n",
        "#                            ]\n",
        "#                        })\n",
        "#                        st.bar_chart(sentiment_df.set_index('指標'))\n",
        "\n",
        "#            # フィードバック\n",
        "#            if not chat.get(\"feedback_rating\"):\n",
        "#                col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#                with col3:\n",
        "#                    st.write(\"この回答はいかがでしたか？\")\n",
        "\n",
        "#                col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#                with col1:\n",
        "#                    if st.button(\"👎 悪い\", key=f\"bad_{chat['id']}\"):\n",
        "#                        st.session_state.database.save_feedback(chat[\"id\"], 1)\n",
        "#                        st.experimental_rerun()\n",
        "#                with col2:\n",
        "#                    if st.button(\"👍 普通\", key=f\"fair_{chat['id']}\"):\n",
        "#                        st.session_state.database.save_feedback(chat[\"id\"], 3)\n",
        "#                        st.experimental_rerun()\n",
        "#                with col3:\n",
        "#                    if st.button(\"👍👍 良い\", key=f\"good_{chat['id']}\"):\n",
        "#                        st.session_state.database.save_feedback(chat[\"id\"], 5)\n",
        "#                        st.experimental_rerun()\n",
        "#            else:\n",
        "#                st.success(f\"評価: {'👍' * (chat['feedback_rating'] // 2)}\")\n",
        "\n",
        "#    # 新しいメッセージの入力\n",
        "#    user_input = st.chat_input(\"メッセージを入力してください\")\n",
        "\n",
        "#    if user_input:\n",
        "#        # ユーザーメッセージの表示\n",
        "#        with st.chat_message(\"user\"):\n",
        "#            st.write(user_input)\n",
        "\n",
        "#        # アシスタントメッセージの表示\n",
        "#        with st.chat_message(\"assistant\"):\n",
        "#            message_placeholder = st.empty()\n",
        "\n",
        "#            # ストリーミング出力のハンドラ\n",
        "#            if STREAM_OUTPUT:\n",
        "#                full_response = \"\"\n",
        "\n",
        "#                # ストリーミングコールバック\n",
        "#                def stream_handler(new_text):\n",
        "#                    nonlocal full_response\n",
        "#                    full_response += new_text\n",
        "#                    message_placeholder.markdown(full_response + \"▌\")\n",
        "\n",
        "#                # 応答生成\n",
        "#                start_time = time.time()\n",
        "#                response, gen_time = st.session_state.llm_generator.generate_text(\n",
        "#                    user_input,\n",
        "#                    max_length=max_length,\n",
        "#                    temperature=temperature,\n",
        "#                    stream_handler=stream_handler\n",
        "#                )\n",
        "\n",
        "#                # ストリーミング出力を最終更新\n",
        "#                message_placeholder.markdown(full_response)\n",
        "#            else:\n",
        "#                # 非ストリーミング出力\n",
        "#                start_time = time.time()\n",
        "#                with st.spinner(\"考え中...\"):\n",
        "#                    response, gen_time = st.session_state.llm_generator.generate_text(\n",
        "#                        user_input,\n",
        "#                        max_length=max_length,\n",
        "#                        temperature=temperature\n",
        "#                    )\n",
        "#                message_placeholder.markdown(response)\n",
        "\n",
        "#            # メトリックスの計算\n",
        "#            metrics = st.session_state.metrics_calculator.calculate_all_metrics(\n",
        "#                user_input, response, gen_time\n",
        "#            )\n",
        "\n",
        "#            # チャット履歴をデータベースに保存\n",
        "#            chat_id = st.session_state.database.save_chat(\n",
        "#                st.session_state.session_id,\n",
        "#                user_input,\n",
        "#                response,\n",
        "#                gen_time,\n",
        "#                metrics\n",
        "#            )\n",
        "\n",
        "#            # チャット履歴をセッションに保存\n",
        "#            st.session_state.chat_history.append({\n",
        "#                \"id\": chat_id,\n",
        "#                \"user_input\": user_input,\n",
        "#                \"model_response\": response,\n",
        "#                \"timestamp\": datetime.now().isoformat(),\n",
        "#                \"metrics\": metrics\n",
        "#            })\n",
        "\n",
        "#            # フィードバックUI\n",
        "#            with st.expander(\"パフォーマンス指標\"):\n",
        "#                cols = st.columns(3)\n",
        "#                with cols[0]:\n",
        "#                    st.metric(\"応答時間\", f\"{gen_time:.2f}秒\")\n",
        "#                with cols[1]:\n",
        "#                    st.metric(\"トークン生成速度\", f\"{metrics.get('token_generation_speed', 0):.1f} t/s\")\n",
        "#                with cols[2]:\n",
        "#                    sentiment = metrics.get('sentiment', {}).get('vader_compound', 0)\n",
        "#                    st.metric(\"感情スコア\", f\"{sentiment:.2f}\", delta=sentiment)\n",
        "\n",
        "#                # 詳細メトリクス\n",
        "#                if 'sentiment' in metrics:\n",
        "#                    sentiment = metrics['sentiment']\n",
        "#                    st.write(\"**感情分析:**\")\n",
        "#                    sentiment_df = pd.DataFrame({\n",
        "#                        '指標': ['ポジティブ', 'ネガティブ', '中立', '主観性'],\n",
        "#                        '値': [\n",
        "#                            sentiment.get('vader_positive', 0),\n",
        "#                            sentiment.get('vader_negative', 0),\n",
        "#                            sentiment.get('vader_neutral', 0),\n",
        "#                            sentiment.get('subjectivity', 0)\n",
        "#                        ]\n",
        "#                    })\n",
        "#                    st.bar_chart(sentiment_df.set_index('指標'))\n",
        "\n",
        "#            col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#            with col3:\n",
        "#                st.write(\"この回答はいかがでしたか？\")\n",
        "\n",
        "#            col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#            with col1:\n",
        "#                if st.button(\"👎 悪い\", key=f\"bad_{chat_id}\"):\n",
        "#                    st.session_state.database.save_feedback(chat_id, 1)\n",
        "#                    st.experimental_rerun()\n",
        "#            with col2:\n",
        "#                if st.button(\"👍 普通\", key=f\"fair_{chat_id}\"):\n",
        "#                    st.session_state.database.save_feedback(chat_id, 3)\n",
        "#                    st.experimental_rerun()\n",
        "#            with col3:\n",
        "#                if st.button(\"👍👍 良い\", key=f\"good_{chat_id}\"):\n",
        "#                    st.session_state.database.save_feedback(chat_id, 5)\n",
        "#                    st.experimental_rerun()\n",
        "\n",
        "# # 履歴ページ\n",
        "# elif page == \"📚 履歴\":\n",
        "#    st.title(\"チャット履歴\")\n",
        "\n",
        "#    # 履歴の表示\n",
        "#    all_history = st.checkbox(\"すべての履歴を表示\", value=False)\n",
        "\n",
        "#    # ページネーション\n",
        "#    total_chats = st.session_state.database.get_total_chat_count(\n",
        "#        None if all_history else st.session_state.session_id\n",
        "#    )\n",
        "\n",
        "#    items_per_page = 5\n",
        "#    total_pages = (total_chats + items_per_page - 1) // items_per_page\n",
        "\n",
        "#    col1, col2, col3 = st.columns([1, 3, 1])\n",
        "#    with col2:\n",
        "#        page_number = st.slider(\"ページ\", 1, max(1, total_pages), 1)\n",
        "\n",
        "#    offset = (page_number - 1) * items_per_page\n",
        "\n",
        "#    history = st.session_state.database.get_chat_history(\n",
        "#        None if all_history else st.session_state.session_id,\n",
        "#        limit=items_per_page,\n",
        "#        offset=offset\n",
        "#    )\n",
        "\n",
        "#    if not history:\n",
        "#        st.info(\"履歴がありません。チャットを始めましょう！\")\n",
        "#    else:\n",
        "#        # CSV/JSONエクスポート機能\n",
        "#        if ENABLE_CHAT_EXPORT:\n",
        "#            col1, col2 = st.columns(2)\n",
        "\n",
        "#            with col1:\n",
        "#                # CSVエクスポート\n",
        "#                csv_data = io.StringIO()\n",
        "#                history_df = pd.DataFrame([{\n",
        "#                    'timestamp': item['timestamp'],\n",
        "#                    'user_input': item['user_input'],\n",
        "#                    'model_response': item['model_response'],\n",
        "#                    'response_time': item['response_time'],\n",
        "#                    'rating': item['feedback_rating'] or 0\n",
        "#                } for item in history])\n",
        "\n",
        "#                history_df.to_csv(csv_data, index=False)\n",
        "\n",
        "#                st.download_button(\n",
        "#                    label=\"CSVとしてエクスポート\",\n",
        "#                    data=csv_data.getvalue(),\n",
        "#                    file_name=f\"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
        "#                    mime=\"text/csv\"\n",
        "#                )\n",
        "\n",
        "#            with col2:\n",
        "#                # JSONエクスポート\n",
        "#                json_data = json.dumps([{\n",
        "#                    'timestamp': item['timestamp'],\n",
        "#                    'user_input': item['user_input'],\n",
        "#                    'model_response': item['model_response'],\n",
        "#                    'response_time': item['response_time'],\n",
        "#                    'metrics': item['metrics'],\n",
        "#                    'rating': item['feedback_rating']\n",
        "#                } for item in history], indent=2)\n",
        "\n",
        "#                st.download_button(\n",
        "#                    label=\"JSONとしてエクスポート\",\n",
        "#                    data=json_data,\n",
        "#                    file_name=f\"chat_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n",
        "#                    mime=\"application/json\"\n",
        "#                )\n",
        "\n",
        "#        # 履歴表示\n",
        "#        for item in history:\n",
        "#            with st.expander(f\"{item['timestamp'][:19]} - {item['user_input'][:50]}...\"):\n",
        "#                st.subheader(\"ユーザー入力\")\n",
        "#                st.write(item['user_input'])\n",
        "\n",
        "#                st.subheader(\"モデル応答\")\n",
        "#                st.write(item['model_response'])\n",
        "\n",
        "#                # メトリクス表示\n",
        "#                col1, col2, col3 = st.columns(3)\n",
        "\n",
        "#                with col1:\n",
        "#                    st.metric(\"応答時間\", f\"{item['response_time']:.2f}秒\")\n",
        "\n",
        "#                with col2:\n",
        "#                    if 'metrics' in item and item['metrics']:\n",
        "#                        metrics = item['metrics']\n",
        "#                        if 'token_generation_speed' in metrics:\n",
        "#                            st.metric(\"生成速度\", f\"{metrics['token_generation_speed']:.1f} t/s\")\n",
        "\n",
        "#                with col3:\n",
        "#                    if item['feedback_rating']:\n",
        "#                        st.metric(\"評価\", f\"{item['feedback_rating']}/5\")\n",
        "#                    else:\n",
        "#                        st.info(\"評価なし\")\n",
        "\n",
        "#                # 感情分析表示\n",
        "#                if 'metrics' in item and item['metrics'] and 'sentiment' in item['metrics']:\n",
        "#                    sentiment = item['metrics']['sentiment']\n",
        "#                    st.subheader(\"感情分析\")\n",
        "\n",
        "#                    sentiment_df = pd.DataFrame({\n",
        "#                        '指標': ['ポジティブ', 'ネガティブ', '中立', '主観性'],\n",
        "#                        '値': [\n",
        "#                            sentiment.get('vader_positive', 0),\n",
        "#                            sentiment.get('vader_negative', 0),\n",
        "#                            sentiment.get('vader_neutral', 0),\n",
        "#                            sentiment.get('subjectivity', 0)\n",
        "#                        ]\n",
        "#                    })\n",
        "\n",
        "#                    st.bar_chart(sentiment_df.set_index('指標'))\n",
        "\n",
        "# # 分析ページ\n",
        "# elif page == \"📊 分析\":\n",
        "#    st.title(\"パフォーマンス分析\")\n",
        "\n",
        "#    # 統計情報の取得\n",
        "#    stats = st.session_state.database.get_statistics()\n",
        "\n",
        "#    # 基本統計\n",
        "#    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "#    with col1:\n",
        "#        st.metric(\"総チャット数\", stats['total_chats'])\n",
        "\n",
        "#    with col2:\n",
        "#        st.metric(\"平均応答時間\", f\"{stats['avg_response_time']:.2f}秒\")\n",
        "\n",
        "#    with col3:\n",
        "#        st.metric(\"平均評価\", f\"{stats['avg_rating']:.1f}/5\")\n",
        "\n",
        "#    # 評価分布\n",
        "#    st.subheader(\"評価分布\")\n",
        "\n",
        "#    rating_df = pd.DataFrame({\n",
        "#        '評価': list(stats['rating_distribution'].keys()),\n",
        "#        '数': list(stats['rating_distribution'].values())\n",
        "#    })\n",
        "\n",
        "#    if not rating_df.empty:\n",
        "#        fig, ax = plt.subplots()\n",
        "#        ax.bar(rating_df['評価'], rating_df['数'])\n",
        "#        ax.set_xlabel('評価点数')\n",
        "#        ax.set_ylabel('回数')\n",
        "#        ax.set_xticks(range(1, 6))\n",
        "#        ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "#        st.pyplot(fig)\n",
        "#    else:\n",
        "#        st.info(\"まだ評価データがありません。\")\n",
        "\n",
        "#    # 時系列分析\n",
        "#    st.subheader(\"応答時間の推移\")\n",
        "\n",
        "#    # 直近20件のチャット取得\n",
        "#    history = st.session_state.database.get_chat_history(limit=20)\n",
        "\n",
        "#    if history:\n",
        "#        time_df = pd.DataFrame([{\n",
        "#            'timestamp': item['timestamp'],\n",
        "#            'response_time': item['response_time']\n",
        "#        } for item in history])\n",
        "\n",
        "#        time_df['timestamp'] = pd.to_datetime(time_df['timestamp'])\n",
        "#        time_df = time_df.sort_values(by='timestamp')\n",
        "\n",
        "#        st.line_chart(time_df.set_index('timestamp'))\n",
        "#    else:\n",
        "#        st.info(\"時系列データがありません。\")\n",
        "\n",
        "#    # モデル性能指標の比較\n",
        "#    st.subheader(\"感情分析\")\n",
        "\n",
        "#    # 感情分析の平均値を計算\n",
        "#    sentiment_data = []\n",
        "\n",
        "#    for item in history:\n",
        "#        if 'metrics' in item and item['metrics'] and 'sentiment' in item['metrics']:\n",
        "#            sentiment = item['metrics']['sentiment']\n",
        "#            sentiment_data.append({\n",
        "#                'timestamp': item['timestamp'],\n",
        "#                'positive': sentiment.get('vader_positive', 0),\n",
        "#                'negative': sentiment.get('vader_negative', 0),\n",
        "#                'neutral': sentiment.get('vader_neutral', 0)\n",
        "#            })\n",
        "\n",
        "#    if sentiment_data:\n",
        "#        sentiment_df = pd.DataFrame(sentiment_data)\n",
        "#        sentiment_df['timestamp'] = pd.to_datetime(sentiment_df['timestamp'])\n",
        "#        sentiment_df = sentiment_df.sort_values(by='timestamp')\n",
        "\n",
        "#        # 積み上げグラフ用にデータを変換\n",
        "#        chart_data = pd.DataFrame({\n",
        "#            'Positive': sentiment_df['positive'],\n",
        "#            'Negative': sentiment_df['negative'],\n",
        "#            'Neutral': sentiment_df['neutral']\n",
        "#        }, index=sentiment_df['timestamp'])\n",
        "\n",
        "#        st.area_chart(chart_data)\n",
        "#    else:\n",
        "#        st.info(\"感情分析データがありません。\")"
      ],
      "metadata": {
        "id": "_vh7DCt87ac7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TBQyTTWTELSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee7e8b9-33b9-4ef5-fea0-b7386a596c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "公開URL: https://a466-34-68-20-49.ngrok-free.app\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.68.20.49:8501\u001b[0m\n",
            "\u001b[0m\n",
            "NLTK loaded successfully.\n",
            "2025-04-20 06:02:45.928364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745128966.207966    7139 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745128966.283511    7139 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-20 06:02:46.875317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "config.json: 100% 805/805 [00:00<00:00, 4.19MB/s]\n",
            "model.safetensors.index.json: 100% 24.2k/24.2k [00:00<00:00, 44.9MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/241M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   4% 10.5M/241M [00:00<00:02, 82.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.99G [00:00<01:31, 54.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.99G [00:00<01:06, 75.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  13% 31.5M/241M [00:00<00:01, 114MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.99G [00:00<00:40, 121MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  22% 52.4M/241M [00:00<00:01, 117MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.99G [00:00<00:33, 147MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  30% 73.4M/241M [00:00<00:01, 115MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.99G [00:00<00:37, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.99G [00:00<00:37, 129MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  39% 94.4M/241M [00:00<00:01, 116MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.99G [00:01<00:35, 138MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  48% 115M/241M [00:00<00:01, 118MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.99G [00:01<00:32, 148MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  57% 136M/241M [00:01<00:00, 119MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.99G [00:01<00:31, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.99G [00:01<00:29, 163MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  65% 157M/241M [00:01<00:00, 120MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.99G [00:01<00:29, 161MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  74% 178M/241M [00:01<00:00, 116MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.99G [00:01<00:31, 149MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  83% 199M/241M [00:01<00:00, 119MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.99G [00:01<00:30, 153MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  91% 220M/241M [00:01<00:00, 130MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.99G [00:01<00:30, 153MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 241M/241M [00:02<00:00, 118MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.99G [00:02<00:32, 144MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.99G [00:02<00:27, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.99G [00:02<00:26, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.99G [00:02<00:25, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.99G [00:02<00:24, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.99G [00:02<00:23, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.99G [00:02<00:23, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.99G [00:02<00:23, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.99G [00:03<00:22, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.99G [00:03<00:22, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.99G [00:03<00:22, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.99G [00:03<00:23, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.99G [00:03<00:23, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.99G [00:03<00:23, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.99G [00:03<00:23, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.99G [00:03<00:23, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.99G [00:03<00:22, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.99G [00:04<00:22, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.99G [00:04<00:22, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.99G [00:04<00:21, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.99G [00:04<00:21, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.99G [00:04<00:20, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.99G [00:04<00:20, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.99G [00:04<00:20, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.99G [00:04<00:19, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 870M/4.99G [00:04<00:20, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.99G [00:05<00:19, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 923M/4.99G [00:05<00:19, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.99G [00:05<00:18, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.99G [00:05<00:19, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/4.99G [00:05<00:18, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.99G [00:05<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.99G [00:05<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.99G [00:06<00:18, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.99G [00:06<00:18, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.99G [00:06<00:17, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.99G [00:06<00:17, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.99G [00:06<00:18, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.99G [00:06<00:18, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.99G [00:06<00:18, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.99G [00:07<00:17, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.99G [00:07<00:17, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.99G [00:07<00:17, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.99G [00:07<00:17, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.99G [00:07<00:17, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.99G [00:07<00:17, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.99G [00:07<00:18, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.99G [00:07<00:19, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.99G [00:08<00:19, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.52G/4.99G [00:08<00:19, 181MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.99G [00:08<00:20, 166MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.99G [00:08<00:21, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.99G [00:08<00:21, 161MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.99G [00:08<00:21, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.99G [00:09<00:26, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.99G [00:09<00:39, 84.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.67G/4.99G [00:09<00:41, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.99G [00:09<00:41, 80.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.99G [00:10<00:39, 83.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.72G/4.99G [00:10<00:39, 82.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.99G [00:10<00:38, 83.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.99G [00:10<00:39, 83.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.99G [00:10<00:39, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.99G [00:11<00:50, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.99G [00:11<00:52, 61.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.99G [00:11<00:46, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.99G [00:11<00:49, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.99G [00:11<00:59, 53.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.99G [00:11<00:59, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.99G [00:12<01:00, 52.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.99G [00:12<00:51, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.99G [00:12<00:55, 56.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.99G [00:12<00:57, 54.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.99G [00:12<00:43, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.99G [00:13<00:34, 88.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.99G [00:13<00:48, 63.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.99G [00:13<00:38, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.99G [00:13<00:36, 82.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.99G [00:13<00:31, 95.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.99G [00:13<00:28, 104MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.99G [00:14<00:29, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.99G [00:14<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.99G [00:14<00:27, 107MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.07G/4.99G [00:14<00:25, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.99G [00:14<00:23, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.99G [00:15<00:29, 98.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.99G [00:15<00:27, 105MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.99G [00:15<00:27, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.99G [00:15<00:25, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.99G [00:15<00:23, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.99G [00:16<00:22, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.99G [00:16<00:21, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.99G [00:16<00:21, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.99G [00:16<00:21, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.99G [00:16<00:20, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.32G/4.99G [00:16<00:20, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.99G [00:16<00:20, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.99G [00:19<01:40, 26.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.99G [00:19<01:26, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.99G [00:19<01:04, 40.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.41G/4.99G [00:19<00:48, 52.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.99G [00:19<00:39, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.99G [00:19<00:32, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.99G [00:20<00:27, 90.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.99G [00:20<00:25, 99.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.52G/4.99G [00:20<00:23, 107MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.99G [00:20<00:22, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.99G [00:20<00:21, 112MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.99G [00:20<00:20, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.99G [00:21<00:20, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.99G [00:21<00:19, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.99G [00:21<00:18, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.66G/4.99G [00:21<00:18, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.99G [00:21<00:18, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.99G [00:22<00:19, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.99G [00:22<00:19, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.99G [00:22<00:19, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.77G/4.99G [00:22<00:19, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.99G [00:22<00:18, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.81G/4.99G [00:25<01:37, 22.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.99G [00:25<01:12, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.99G [00:25<00:57, 37.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.86G/4.99G [00:26<00:54, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.99G [00:26<00:40, 51.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.99G [00:26<00:32, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.99G [00:26<00:27, 74.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.99G [00:26<00:27, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.97G/4.99G [00:27<00:23, 85.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.99G [00:27<00:27, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.01G/4.99G [00:27<00:24, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.99G [00:27<00:21, 91.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.99G [00:27<00:18, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.99G [00:28<00:17, 111MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.99G [00:28<00:16, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/4.99G [00:28<00:16, 116MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.99G [00:28<00:15, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.99G [00:28<00:14, 123MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.99G [00:28<00:14, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.99G [00:29<00:25, 69.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.99G [00:29<00:21, 82.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.99G [00:29<00:18, 92.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.26G/4.99G [00:29<00:16, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.99G [00:30<00:15, 111MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.99G [00:30<00:14, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.99G [00:30<00:14, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.99G [00:30<00:13, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.37G/4.99G [00:30<00:13, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.99G [00:30<00:12, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.41G/4.99G [00:31<00:12, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.99G [00:31<00:11, 133MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.99G [00:31<00:11, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.99G [00:31<00:11, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.99G [00:31<00:11, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.51G/4.99G [00:31<00:11, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.99G [00:32<00:11, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.99G [00:34<01:02, 23.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.57G/4.99G [00:35<01:13, 19.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.99G [00:35<00:51, 27.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.61G/4.99G [00:35<00:37, 36.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.99G [00:36<00:28, 47.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.99G [00:36<00:22, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.99G [00:36<00:18, 71.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.99G [00:36<00:15, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.71G/4.99G [00:36<00:13, 92.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.99G [00:36<00:12, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.99G [00:37<00:11, 107MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.99G [00:37<00:10, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.99G [00:37<00:09, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.99G [00:37<00:09, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.99G [00:37<00:09, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.86G/4.99G [00:37<00:09, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.99G [00:38<00:11, 95.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.99G [00:38<00:10, 105MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.99G [00:38<00:09, 112MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.99G [00:39<00:19, 52.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.95G/4.99G [00:39<00:18, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.99G [00:39<00:14, 70.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.99G [00:39<00:11, 84.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.99G [00:39<00:10, 96.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.99G [00:40<00:08, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.06G/4.99G [00:40<00:08, 113MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.99G [00:40<00:07, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.99G [00:40<00:07, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.99G [00:40<00:06, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.99G [00:40<00:06, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.16G/4.99G [00:41<00:06, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.99G [00:41<00:06, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.20G/4.99G [00:41<00:06, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.99G [00:41<00:05, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.99G [00:41<00:05, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.99G [00:41<00:05, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.99G [00:42<00:05, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.31G/4.99G [00:45<00:40, 16.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.99G [00:46<00:29, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.35G/4.99G [00:46<00:21, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.99G [00:46<00:15, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.99G [00:46<00:12, 49.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.99G [00:46<00:09, 60.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.99G [00:46<00:07, 71.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.46G/4.99G [00:47<00:08, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.99G [00:47<00:06, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.99G [00:47<00:05, 83.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.99G [00:47<00:04, 94.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.99G [00:47<00:04, 102MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.56G/4.99G [00:48<00:03, 108MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.99G [00:48<00:03, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.60G/4.99G [00:48<00:03, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.99G [00:48<00:02, 125MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.99G [00:48<00:02, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.99G [00:52<00:17, 18.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.99G [00:52<00:11, 25.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.71G/4.99G [00:52<00:08, 33.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.99G [00:52<00:05, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.75G/4.99G [00:52<00:04, 55.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.99G [00:52<00:03, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.99G [00:53<00:02, 75.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.81G/4.99G [00:53<00:02, 86.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.99G [00:53<00:01, 96.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.85G/4.99G [00:53<00:01, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.99G [00:53<00:00, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.99G [00:53<00:00, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.99G [00:54<00:00, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.99G [00:54<00:00, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.96G/4.99G [00:54<00:00, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:54<00:00, 91.3MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:54<00:00, 27.46s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 10.58it/s]\n",
            "generation_config.json: 100% 168/168 [00:00<00:00, 1.06MB/s]\n",
            "tokenizer_config.json: 100% 46.9k/46.9k [00:00<00:00, 14.4MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 43.1MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 147MB/s]\n",
            "special_tokens_map.json: 100% 555/555 [00:00<00:00, 3.55MB/s]\n",
            "Device set to use cpu\n",
            "2025-04-20 06:04:08.189 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"公開URL: {public_url}\")\n",
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxliKOTawSDe"
      },
      "source": [
        "アプリケーションの機能としては、チャット機能や履歴閲覧があります。\n",
        "\n",
        "これらの機能を実現するためには、StreamlitによるUI部分だけではなく、SQLiteを使用したチャット履歴の保存やLLMのモデルを呼び出した推論などの処理を組み合わせることで実現しています。\n",
        "\n",
        "- **`app.py`**: アプリケーションのエントリーポイント。チャット機能、履歴閲覧、サンプルデータ管理のUIを提供します。\n",
        "- **`ui.py`**: チャットページや履歴閲覧ページなど、アプリケーションのUIロジックを管理します。\n",
        "- **`llm.py`**: LLMモデルのロードとテキスト生成を行うモジュール。\n",
        "- **`database.py`**: SQLiteデータベースを使用してチャット履歴やフィードバックを保存・管理します。\n",
        "- **`metrics.py`**: BLEUスコアやコサイン類似度など、回答の評価指標を計算するモジュール。\n",
        "- **`data.py`**: サンプルデータの作成やデータベースの初期化を行うモジュール。\n",
        "- **`config.py`**: アプリケーションの設定（モデル名やデータベースファイル名）を管理します。\n",
        "- **`requirements.txt`**: このアプリケーションを実行するために必要なPythonパッケージ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvm8sWFPELSP"
      },
      "source": [
        "後片付けとして、使う必要のないngrokのトンネルを削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WFJC2TmZELSP"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUXhIzV7ELSP"
      },
      "source": [
        "# 03_FastAPI\n",
        "\n",
        "ディレクトリ「03_FastAPI」に移動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ejjDLxr3kfC"
      },
      "outputs": [],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/03_FastAPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45TDsNzELSQ"
      },
      "source": [
        "必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uv6glCz5a7Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfrmE2VmELSQ"
      },
      "source": [
        "ngrokとhuggigfaceのトークンを使用して、認証を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELzWhMFORRIO"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wztc2CELSQ"
      },
      "source": [
        "アプリを起動します。\n",
        "\n",
        "「02_streamlit_app」から続けて「03_FastAPI」を実行している場合は、モデルのダウンロードが済んでいるため、すぐにサービスが立ち上がります。\n",
        "\n",
        "「03_FastAPI」のみを実行している場合は、初回の起動時にモデルのダウンロードが始まるので、モデルのダウンロードが終わるまで数分間待ちましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meQ4SwISn3IQ"
      },
      "outputs": [],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLubjIhbELSR"
      },
      "source": [
        "FastAPIが起動すると、APIとクライアントが通信するためのURL（エンドポイント）が作られます。\n",
        "\n",
        "URLが作られるのと合わせて、Swagger UIというWebインターフェースが作られます。\n",
        "\n",
        "Swagger UIにアクセスすることで、APIの仕様を確認できたり、APIをテストすることができます。\n",
        "\n",
        "Swagger UIを利用することで、APIを通してLLMを動かしてみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgumW3mGELSR"
      },
      "source": [
        "後片付けとして、使う必要のないngrokのトンネルを削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJymTZio-WPJ"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}